%!TEX root = ../thesis.tex

% **************************** Define Graphics Path **************************
\ifpdf
\graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
\graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\chapter{Feynman-Kac: connecting Quantum Mechanics and Stochastic Processes}
\label{chapter3}
In this chapter we will provide a bridge between the quantum many-body problem discussed in the previous chapter and stochastic processes. This will entail introducing the \textbf{Feynman-Kac} (FK) formula and relating it to the \textbf{Fokker-Planck} (FP) equation and optimal control formulations of QM. Moreover, a probabilistic view of the cost function will lead us to proposals for loss functions that can be used to learn optimal transition rates and consequently sample the ground state.

The field of stochastic processes is a vast body of work, approached from different angles by mathematicians, physicists and engineers. A necessary consequence of this is that the literature ranges from extremely thorough and rigorous~\cite{rogers1994diffusions, rogers2000diffusions} to more applied and intuitive~\cite{sarkka2019applied}. For this reason, the mentioned discussion will be preceded by an overview of the mathematical notation, lemmas and results from stochastic processes and measure theory that underpin some core ideas of this thesis. Some concepts will not be rigorously defined, the text will point to relevant literature where this is the case.

\section{Stochastic processes}
\label{subsec:fk-stoch}
\subsection{Fundamentals}
This brief, more formal, discussion of stochastic processes is based mostly upon classic texts~\cite{durrett2019probability, rogers1994diffusions, rogers2000diffusions} and borrows some intuitions from~\cite{sarkka2019applied}. The most basic quantity that we will need is the \textbf{probability space}. 
\begin{definition}[Probability space]
	The probability space is a tuple $(\Omega, \mathcal{F}, \mathbb{P})$, where $\Omega$ is the \textbf{sample space}, $\mathcal{F}$ is a $\sigma$-\textbf{field}, and $\mathbb{P}$ is the \textbf{measure}.
\end{definition}
The sample space is simply the set of all possible outcomes. A canonical example would be the roll of a 6-sided dice, $\Omega=\{1, 2, 3, 4, 5, 6\}$. Without measure $\mathbb{P}$, the tuple $(\Omega, \mathcal{F})$ is termed a \textbf{measurable space}.
\begin{definition}[$\sigma$-field]
	A $\sigma$-field $\mathcal{F}$ on a set $\Omega$, is a nonempty collection of subsets of $\Omega$ that includes $\Omega$ itself, is closed under complement, i.e. if $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$, and is closed under countable unions, $\cup_{i} A_{i} \in \mathcal{F}$ if $A_{i} \in \mathcal{F}$ is a countable union of sets.
\end{definition}
The main utility of the $\sigma$-field to us is its use in defining measures. We want to be able to assign a non-negative real number to all subsets of $\Omega$, as well as the size of the union of the disjoint sets to be the sum of their individual sizes. This is not always possible, a counterexample for the real line being Vitali sets. The collection $\mathcal{F}$, must thus only include \emph{measurable} sets, which are precisely the ones that satisfy the constraints imposed by the $\sigma$-field.
\begin{definition}[Measure]
	A non-negative countably additive set function $\mu: \mathcal{F} \rightarrow \mathbb{R}$ that satisfies
	\begin{enumerate}[label=\roman*)]
		\item $\mu(A) \geq \mu(\emptyset)=0$ for all $A \in \mathcal{F}$
		\item if $A_{i} \in \mathcal{F}$ is a countable sequence of disjoint sets, then $\mu\left(\cup_{i} A_{i}\right)=\sum_{i} \mu\left(A_{i}\right)$
	\end{enumerate}
	is a \textbf{measure}.
\end{definition}
If $\mu(\Omega)=1$, then $\mu$ is a \textbf{probability measure} and will be denoted by $\mathbb{P}$. With this notion we are now able to define a random variable (r.v) and a stochastic process (s.p.).
\begin{definition}[Random variable]
	 A \textbf{random variable} $X$ defined on $\Omega$ is a real-valued measurable function $X(\omega)$, $X: \Omega \rightarrow \mathbb{R}^d$.
\end{definition}
For a function to be measurable, we require that its preimage $X^{-1}$ is in the $\sigma$-field $\mathcal{F}$
\begin{equation}
	X^{-1}(B) = \{ \omega : X(\omega) \in B \} \in \mathcal{F},
\end{equation}
and that this holds for every Borel set $B$ in the Borel $\sigma$-field~\footnote{For a proper definition of the Borell set see ch. 3 of~\cite{salamon2016measure}.} of $\mathbb{R}^d$, which is simply the smallest $\sigma$-field that contains all measurable sets in $\mathbb{R}^d$. 

A random variable $X$ induces a probability measure $\mu$ on $\mathbb{R}^d$ called its \textbf{distribution}, this is done by setting $\mu(A)=P(X \in A)$ for Borel sets $A$. Moreover, the distribution is usually given in terms of a \textbf{distribution function} $F(x)$
\begin{equation}
	 F(x) = \mathbb{P}(\{\omega \in \Omega: X(\omega) \leq x\}) = \mathbb{P}(X \leq x),
\end{equation}
and $X$ is said to have a \textbf{density function} $f(x)$ if $F(x)$ can be written as 
\begin{equation}
	\label{eq:pdf}
	F(x)=\int_{-\infty}^{x} f(y) \mathrm{d}y.
\end{equation}
In essence, the random variable provides a connection between the less familiar probability measure $\mathbb{P}$ and the cumulative distribution function (CDF).

\subsection{Stochastic process}
\begin{definition}[Stochastic process]
		Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a measurable (state) space $(E, \mathcal{E})$, we define the collection $\left\{X_{t}: t \in T\right\}$ of set $T$ indexed and $(E, \mathcal{E})$ valued random variables a \textbf{stochastic process}.
\end{definition}
By far the most common case for the index set $T$ is time $T = \mathbb{R}^+$. Such s.p's are called \emph{temporal}, with examples including the model of velocity of a Brownian particle under influence of friction, in Fig.~\ref{fig:sp-brown}, or the Black-Scholes model. Nevertheless, the index set is not limited to time, as is often the case with Gaussian Process regression~\cite{rasmussen2006gaussian}. In this thesis we will mostly deal with temporal s.p's of the kind that do not "see into the future". This notion is formalized using \textbf{filtrations}. A filtration $\mathbb{F}=\left(\mathcal{F}_{t}\right)_{t \in T}$ on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is just an increasing sequence or order of $\sigma$-fields
\begin{equation}
	\mathcal{F}_{s} \subset \mathcal{F}_{t} \text { if } 0 \leq s \leq t<\infty.
\end{equation}
The filtration associated to a process that records its "past behaviour" at each time is called the \textbf{natural filtration}. 
\begin{definition}[Adapted process]
	A process $\{X_t\}$ is said to be \textbf{adapted to the filtration} $\left(\mathcal{F}_{t}\right)_{t \in T}$ if the random variable $X_t : \Omega \rightarrow E$ is $\mathcal{F}_t$-measurable function for each $t \in T$. 
\end{definition}
A process that is \emph{non-anticipating}, i.e. depends only on the past and present, is adapted to the filtration $\left(\mathcal{F}_{t}\right)_{t \in T}$.
\begin{definition}[Brownian motion]
	\textbf{Brownian motion} or a non-anticipating \textbf{Wiener process} is a stochastic process $W_t$, with the following properties:
	\begin{enumerate}[label=\roman*)]
		\item $W_0 = 0$
		\item $W_t$ is almost surely continuous in t
		\item $W_t$ has independent increments
		\item $W_t - W_s \sim \mathcal{N} (0, t-s)$ for $0 \leq s \leq t$
	\end{enumerate}
\end{definition}
\noindent
A realisation of Brownian motion can be found in Fig.~\ref{fig:sp-brown}.
\begin{figure}[h]
	\centering
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Raster/OU-BM-I.png}}
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Raster/OU-BM-II.png}}
	\caption[Brownian motion and Ornstein–Uhlenbeck process
	]{\textbf{Brownian motion and Ornstein–Uhlenbeck process.} 
		A single realisation of the Brownian process (\textbf{left}). The mean, variance and 10 samples of the Ohrnstein-Uhlenbeck process with $\theta=0.6, \sigma=1.1, X_0=1.0, \mu=-1.3$, integrated using Euler-Maruyama method (\textbf{right}).}
	\label{fig:sp-brown}
\end{figure}

\subsection{Integrals}
In order to proceed and define \textbf{stochastic differential equations} (SDE's) and the \textbf{Radon-Nikodyn} (RN) derivative, we must spend some time discussing various integrals we will use. In particular, alongside the usual Riemann integral, we will need three more types of integrals, which we will briefly describe without mathematical derivation. The simplest kind of integral we will introduce is the integral of a stochastic process
\begin{equation}
	I = \int_{0}^{t} X_t \mathrm{d}t.
\end{equation}
The simple appearance of the integral is deceiving as the integrand is a realisation of a $\mathcal{F}_t$-adapted stochastic process $\{X_t\}: \Omega \times T \rightarrow \mathbb{R}^{d}$, meaning that $I$ itself is a random variable. However, since each realisation of $X_t$ is almost surely continuous, $I$ can be expanded as a Riemann sum, which converges under mean-squared norm to $I$, so long as the mean $\mathbf{m}(t) = \mathbb{E}[X_t]$ and covariance $\textbf{k}(t,s) = \operatorname{Cov}(X_t, X_s)$ are continuous. In practice, computing the mean and covariance of $I$ is usually enough to understand the resulting stochastic process. Importantly, integrals of continuous functions of s.p's $h(X_t),~ h: \mathbb{R} \rightarrow \mathbb{R}$ can be computed in a similar manner.

The second type of integrals we need to consider, are integrals with respect to a s.p, known as \textbf{It\^ o integrals}
\begin{equation}
	\label{eq:ito}
	Y_t = \int_{0}^{t} H_s \mathrm{d}X_s,
\end{equation}
where both $H_s$ and $X_s$ are stochastic processes. The result integral $Y_t$ is itself a stochastic process which resides in the probability space $(\Omega, \mathcal{F}, \mathbb{P})$, filtered by $\left(\mathcal{F}_t\right)_{t\in T}$. The integral can be formalised by putting slight constraints on the sort of stochastic processes $X_s$ and $H_t$ can be, expanding $Y_t$ as a Riemann sum and proving convergence. Details of this procedure can be found in~\cite{rogers2000diffusions}.

Finally we must define the \textbf{Lebesgue-Stieltjes integral}~\cite{halmos2013measure}, which is required to properly define expectations of stochastic processes. 
\begin{definition}[Lebesgue-Stieltjes Integral]
		Given probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and measurable function $f:\Omega \rightarrow \mathbb{R}$, the \textbf{Lebesgue-Stieltjes integral} 
		\begin{equation}
			I = \int_A f(x) \mathrm{d}\mathbb{P}(x),
		\end{equation}
		is the Lebesgue integral~\footnote{For proper definition of the Lebesgue integral see ch. 1 of~\cite{salamon2016measure}.} with respect to measure $\mathbb{P}$, $A \in \mathcal{F}$.
\end{definition}
With it we can define expectations in the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ as
\begin{equation}
	\underset{{\mathbb{P}}}{\mathbb{E}}[f(x)]=\int_{\Omega} f(x) d \mathbb{P}(\boldsymbol{x}).
\end{equation}
For a newcomer to stochastic processes this formulation may seem redundant, can we not just calculate expectations using a Riemann integral and the pdf? We can, and when the distribution $\mathbb{P}$ can be expressed in terms of the pdf~\eqref{eq:pdf}, the Lebesgue integral can be interpreted in this way. However, stochastic processes need not admit a pdf, that is when the Lebesgue-Stieltjes integral is necessary.
%\begin{equation}
%	\int_{\Omega} f(x) \mathrm{d} \mathbb{P}(x)=\int_{\Omega} f(x) p(x) d \lambda(x)
%\end{equation}
\subsection{Stochastic Differential Equations}
In this thesis we will refer to an SDE as an informal notation of an It\^ o integral equation or \textbf{It\^ o process}. 
\begin{definition}[It\^ o process]
	Given deterministic functions $v: \mathbb{R}^{d} \times \mathbb{R}^{+} \rightarrow \mathbb{R}^{d}$ and $\sigma: \mathbb{R}^{d} \times \mathbb{R}^{+} \rightarrow \mathbb{R}^{d \times d}$, we define the \textbf{It\^ o process} $X_t$ as the sum of It\^ o and Lebesgue integrals
	\begin{equation}
		\label{eq:ito-process}
		X_{t+s}-X_{t}=\int_{t}^{t+s} \sigma\left(X_{u}, u\right) \mathrm{d} W_{u} + \int_{t}^{t+s} v\left(X_{u}, u\right) \mathrm{d} u,
	\end{equation}
	where $W_t$ is a Brownian motion.
\end{definition}
In simplified notation eq.~\eqref{eq:ito-process} can be written as
\begin{equation}
	\label{eq:SDE_general}
	\mathrm{d} X_t = \sigma \left(X_{t}, t\right)\mathrm{d}W_t + v\left(X_{t}, t\right) \mathrm{d}t,
\end{equation}
this is what we refer to as an SDE, an example can be found in Fig.~\ref{fig:sp-brown}. We will refer to functions $v$ and $\sigma$, as the \textbf{drift} and \textbf{volatility} of an It\^ o process. The most intuitive interpretation of an SDE is in terms of the time evolution of the pdf of the process $X_t$. It is described by the \textbf{Fokker-Planck equation}~\footnote{Derivation in~\cite{sarkka2019applied}.} 
\begin{equation}
	\label{eq:Fokker-Planck-General}
	\frac{\partial p(\mathbf{x}, t)}{\partial t}=
	-\sum_{i=1}^{N} \frac{\partial}{\partial x_{i}}\left[\mu_{i}(\mathbf{x}, t) p(\mathbf{x}, t)\right]
	+\sum_{i=1}^{N} \sum_{j=1}^{N} \frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\left[D_{i j}(\mathbf{x}, t) p(\mathbf{x}, t)\right],
\end{equation}
where $p(\mathbf{x}, t)$ is the pdf of the solution to the SDE and $\mathbf{D}=\frac{1}{2} \boldsymbol{\sigma} \boldsymbol{\sigma}^{\top}$ is the diffusion tensor. Finally we state without proof a consequence of It\^ o calculus, most commonly named \textbf{It\^ o's rule} or \textbf{lemma}. It is the stochastic calculus equivalent of the chain rule
\begin{lemma}[It\^ o's lemma]
	Given an It\^ o process $X_t$ as given by ~\eqref{eq:ito-process} and a twice differentiable scalar function $f(X_t, t)$, then the It\^ o process for $f$ is
	\begin{equation}
		\label{eq:ito_lemma}
		\mathrm{d} f = \frac{\partial f}{\partial t}\mathrm{d}t + \sum_i \frac{\partial f}{\partial x_i} \mathrm{d}x_i + \frac{1}{2}\sum_{ij} \frac{\partial^{2}}{\partial x_{i} \partial x_{j}} f \mathrm{d}x_i \mathrm{d}x_j,
	\end{equation}
\end{lemma}
when compared to ordinary calculus we notice an additional quadratic term.

\subsection{Radon-Nikodym Derivative and Girsanov theorem}
To perform importance sampling we perform a change of measure in an integral
\begin{equation}
	\int_{A} f(x) \mathrm{d} \mathbb{P}(x)=\int_{A} f(x) \frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{Q}}(x) \mathrm{d} \mathbb{Q}(x).
\end{equation}
The function that measures the rate of change of density of one measure w.r.t another is the \textbf{Radon-Nikodym derivative} $\frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{Q}}(x)$.

\begin{theorem}[Radon-Nikodym theorem]
	Let $\mathbb{P}$ and $\mathbb{Q}$ be probability measures on the measurable space $(\Omega, \mathcal{F})$, then the measurable function \textbf{Radon-Nikodym derivative} $\frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{Q}}(x): \Omega \rightarrow[0, \infty)$ exists and 
	\begin{equation}
		\mathbb{P}(A)=\int_{A} \frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{Q}}(x) \mathrm{d} \mathbb{Q}(x),
	\end{equation}
	for set $A \subseteq \mathcal{F}$.
\end{theorem}
The RN derivative will also be useful in defining the KL divergence between two \textbf{path measures}. Properly defining the path measure would bring a lot of notational overhead. It is enough to think of it as a measure on the \textbf{path space}, i.e all possible paths of a SDE, for rigour see~\cite{leonard2014pathmeasure}. 
Finally, we state \textbf{Girsanov theorem} that is often used for transforming or removing drift functions of SDEs. It is the RN derivative between an It\^ o process and one with $v = 0$ and $\sigma = 1$, i.e. Brownian motion. 
\begin{theorem}[Girsanov Theorem]
	Given It\^ o process
	\begin{equation}
		\mathrm{d}X_t = \mathrm{d}W_t + v(X_t, t)\mathrm{d}t \quad \text{and} \quad X_0 = 0 \\
	\end{equation}
	and Brownian motion $\mathrm{d}Y_t = \mathrm{d}W_t$, the RN derivative of their respective path measures $\mathbb{P}$ and $\mathbb{P}_0$	is
	\begin{equation}
		\label{eq:Girsanov_theorem}
		\frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{P}_0}=
		\exp \left(
		-\frac{1}{2} \int_{0}^{t}\left|v(X_s, s)\right|^{2} \mathrm{d} s+\int_{0}^{t} v(X_s, s)^{\top} \mathrm{d} W_s\right).
	\end{equation}
\end{theorem}

\noindent
This \emph{change in dynamics}, as we will call it, is true in the sense that expectations for an arbitrary functional $h(\cdot)$ of the path from $0$ to $t$ are
\begin{equation}
	\label{eq:girsanov_consequence}
	\underset{\mathbb{P}}{\mathbb{E}}\left[h(X_t)\right] = \underset{\mathbb{P}_0}{\mathbb{E}}\left[\frac{\mathrm{d} \mathbb{P}}{\mathrm{d} \mathbb{P}_0}. h(Y_t)\right]. 
\end{equation}
For a more general case and proof see~\cite{sarkka2019applied}.

\subsection{Markov processes}
We now shift our view to a special kind of s.p's, ones that satisfy the \textbf{Markov property} called \textbf{Markov processes} or \textbf{Markovian}. The property is sometimes referred to as \emph{memorylessness}, as the future of a Markov process depends only on the present state. We can classify the processes based on the system's \textbf{state-space} $S$, which can be either discrete (countable) or continuous, and \textbf{time indexing} of the system, either discrete-time $\{X_n\}_{n \geq 0}$ or continuous-time $\{X_t\}_{t \geq 0}$. A taxonomy is given in Table~\ref{tab:MP-taxonomy}. We will not specifically discuss Markov processes in continuous state-space, but it is important to note that any It\^ o process with time-homogenous drift $v = v(X_t)$ and volatility $\sigma = \sigma(X_t)$ is Markovian.

From now on we refer to Markov processes in countable state-space as \textbf{Markov chains}. We base our discussion on~\cite{rogers1994diffusions} and~\cite{norris1998markov}. 
\begin{table}[h]
	\begin{center}
		\caption[Taxonomy of Markov processes]{\textbf{Taxonomy of Markov processes}}
		\label{tab:MP-taxonomy}
		\begin{tabular}{p{2cm}|p{6cm}|p{6cm}}
%			\toprule % <-- Toprule here
			 & Countable state-space & Continuous state-space\\
			\midrule
			Discrete time 
			&\parbox{5cm}{\textbf{index: }$\{X_n\}_{n \geq 0}$, $n \in \mathbb{Z}^{+}$ \\ \textbf{state-space: } countable set $I$ \\ \textbf{define: } stochastic $\{P\}_{ij}$ \\ \textbf{example:} DTMC}  
			&\parbox{5cm}{\textbf{index: }$\{X_n\}_{n \geq 0}$, $n \in \mathbb{Z}^{+}$ \\ \textbf{state-space: } general state-space $\Omega$ \\ \textbf{define: } stochastic kernel $K$ \\ \textbf{example:} Harris Chain}\\
			\midrule
			Continuous time 
			&\parbox{5cm}{\textbf{index: }$\{X_t\}_{t \geq 0}$, $t \in \mathbb{R}^{+}=[0, \infty)$ \\ \textbf{state-space: } countable set $I$ \\ \textbf{define: } rate $\{\Gamma\}_{ij}$ equiv. to jump chain $\{J_n\}_{n\geq0}$ and hold times $\{S_n\}_{n\geq1}$. \\ \textbf{example:} CTMC}
			&\parbox{5cm}{\textbf{index: }$\{X_t\}_{t \geq 0}$, $t \in \mathbb{R}^{+}=[0, \infty)$ \\ \textbf{state-space:} general state-space $\Omega$ \\ \textbf{define: } stochastic kernel $K$ \\ \textbf{example:} Diffuson process}\\
			\bottomrule % <-- Bottomrule here
		\end{tabular}
	\end{center}
\end{table}
\subsubsection{Discrete-time Markov Chains}
The simplest and most common Markov process is a Markov chain in discrete time, see Fig.~\ref{fig:mc-diagrams}a. Its state-space is a countable set $I$ and we call each $i \in I$ a \textbf{state}. We define a distribution $\lambda$ in a familiar way
\begin{equation}
	\lambda = \{\lambda_i : i \in I\} \quad \text{where} \quad \forall i:~ 0 \leq \lambda_i < \infty \quad \text{and} \quad \sum_{i \in I} \lambda_i = 1.
\end{equation}
We can now set $\lambda$ as a distribution of some random variable $X:\Omega \rightarrow I$ as
\begin{equation}
	\lambda_{i}=\mathbb{P}(X=i)=\mathbb{P}(\{\omega: X(\omega)=i\}),
\end{equation}
where we are still working in the probability space $(\Omega, \mathcal{F}, \mathbb{P})$. A discrete-time Markov chain is defined in terms of its \textbf{transition matrix} $P=\{p_{i j}: i, j \in I\}$, which is a \textbf{stochastic matrix} meaning all of its rows $\{p_{i j}: j \in I\}$ are distributions.
\begin{definition}[Discrete-time Markov chain]
	A discrete time stochastic process $\{X_n\}_{n \geq 0}$ is a \textbf{discrete-time Markov chain} with initial distribution $\lambda$ and transition matrix $P$ if for $i_1, \dots, i_n+1 \in I$ and $n \geq 0$
	\begin{enumerate}[label=\roman*)]
		\item $\mathbb{P}\left(X_{0}=i_{1}\right)=\lambda_{i_{1}}$
		\item $\mathbb{P}\left(X_{n+1}=i_{n+1} \mid X_{0}=i_{0}, \dots, X_{n}=i_{n}\right)=p_{i_{n} i_{n+1}}$
	\end{enumerate}
\end{definition}
After rewriting the second condition above, it is clear that the Markov chain is without memory as
\begin{equation}
	\mathbb{P}\left(X_{n+1}=i_{n+1} \mid X_{0}=i_{1}, \dots, X_{n}=i_{n}\right)=\mathbb{P}(X_{n+1} = i_{n+1} \mid X_{n} = i_{n}).
\end{equation}
Intuitively we understand the discrete-time Markov chain as a system changing its state at discrete time intervals, each time choosing the next state according to the row of the Transition matrix corresponding to the current state.
\begin{figure}[h]
	\centering
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Vector/mc_I.pdf}}
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Vector/mc_II.pdf}}
	\caption[Discrete and continuous time Markov Chains.]{\textbf{Discrete and continuous time Markov Chains.} Discrete-time Markov Chain defined by $P$ (\textbf{left}). Continuous-time Markov Chain defined by $\Gamma$ (\textbf{right}).}
	\label{fig:mc-diagrams}
\end{figure}

\subsubsection{Continuous-time Markov Chains}
\label{subsubsect:CTMC}
Defining a Markov chain in continuous time is trickier, as describing the system with a stochastic matrix no longer suffices due to transition probabilities becoming zero considering an infinitesimal time. Instead a continuous-time Markov Chain (CTMC) is characterised with a \textbf{rate matrix} or \textbf{infinitesimal generator matrix} $\Gamma$ defined on the set $I$, which has the following three properties
\begin{enumerate}[label=\roman*)]
	\item $0 \leq \Gamma_{ii} <\infty, \quad \forall i$
	\item $\Gamma_{ij} \geq 0, \quad \forall i \neq j$
	\item $\sum_{j \in I} \Gamma_{i j}=0, \quad \forall i$.
\end{enumerate}
While the CTMC can be interpreted in a number of ways, we shall use the so called \textbf{jump chain} and \textbf{holding times} representation, shown in Fig.~\ref{fig:holdingJumping}. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{Chapter3/Figs/Vector/ctmc.pdf}
	\caption[Jump chain and Holding times]{\textbf{Jump chain and Holding times.} A discrete space Markov process $\{X_t\}_{t\geq 0}$ in continuous time. The holding times $S_n$ are independent exponential random variables and the transition probabilities at jump times $J_n$ are given with the jump matrix $\Pi$. Adapted from~\cite{norris1998markov}.}
	\label{fig:holdingJumping}
\end{figure}

We can think of a CTMC as a series of discrete jumps, where the system remains in each state for a certain holding time. This suggests that we can construct the CTMC from a discrete-time chain with stochastic matrix $\Pi$, which we will call the \textbf{jump matrix}, and a set of independent random variables $\{S_n\}$ which determine the holding times. We construct matrix $\Pi$ by rescaling rows of $\Gamma$ so they add up to one and putting a $0$ on the diagonal
\begin{equation}
	\begin{array}{l}
		\Pi_{i j}=\left\{
		\begin{array}{ll}
			\Gamma_{i j} / \Gamma_{ii} & \text { if } j \neq i \text { and } \Gamma_{ii} \neq 0 \\
			 0 & \text { if } j \neq i \text { and } \Gamma_{ii}=0
		\end{array}\right. 
		\\ 
		\Pi_{i i}=\left\{
		\begin{array}{ll}
			0 & \qquad  \text { if } \Gamma_{ii} \neq 0 \\
			1 & \qquad \text { if } \Gamma_{ii}=0.
		\end{array}\right.
	\end{array}
\end{equation}
In order for the process to possess the Markov property, the distribution of holding times $\{S_n\}$ must be exponential~\cite{norris1998markov}, 
\begin{equation}
	S_{n+1} \sim \operatorname{Exp}(-{\Gamma_{ii}}(X_n)),
\end{equation}
with exponential parameters being $-\Gamma_{ii}$, where $i$ is the current state.
Processes with different holding time distributions are called \textbf{semi-Markov}. The jump times $\{J_n\}$ are simply 
\begin{equation}
	J_{n}=S_{1}+\ldots+S_{n}.
\end{equation}
\begin{definition}[Continuous-time Markov chain]
	A stochastic process $\{X_t\}_{t \geq 0}$ on set $I$ is a \textbf{continuous-time Markov chain} if its jump chain $\{Y_n\}_{n \geq 0}$ is a discrete-time Markov chain and its holding times $\{S_n\}_{n \geq 1}$ are independent exponential random variables $S_n \sim \operatorname{Exp}(-{\Gamma_{ii}}(X_n))$.
\end{definition}
An equivalent formulation is in terms of \textbf{competing exponentials}. Transitions $\Gamma_{j \rightarrow k}$ from $j$ to $k$ are defined as independent exponential random variables $\tau_{j \rightarrow k}$
\begin{equation}
	\tau_{j \rightarrow k} \sim \operatorname{Exp}(\Gamma_{jk}), \quad j \neq k
\end{equation}
the next state is then chosen as
\begin{equation}
	Y_{n+1} = \operatorname{argmin}_{k} \tau_{j \rightarrow k}.
\end{equation}
The chain $\{Y_n\}_{n\geq0}$ along with times 
\begin{equation}
	S_{n} = \min _{k} \tau_{j \rightarrow k},
\end{equation}
gives the full description of the CTMC. We now interpret $\Gamma_{ii}$ as the rate ob \emph{leaving} current state and $\Gamma_{i j}$ as the rate of \emph{going} from $i$ to $j$.

\section{The Feynman-Kac formula}
\label{subsec:fk-fk}
The Feynman path integral formulation introduced in Chapter~\ref{chapter2} was extensively used by physicists for decades, even in the absence of a formal mathematical formulation which is hard to define because of the difficulties with defining an appropriate measure on the path space. Kac~\cite{kac1949distributions} provided a rigorous formulation of the \textit{real-valued} case of the Feynman path integral, and the resulting {Feynman-Kac formula} eq.~\eqref{eq:fkac} provides a bridge between \emph{parabolic} partial differential equations and stochastic processes. 

\subsection{Feynman-Kac in continuous state space}
\label{subsec:FK_in_continuous_space}
To illustrate the Feynman-Kac formula, let us consider a single particle with Hamiltonian
\begin{equation}
	\hat{H} = -\frac{\mathrm{d}^2~~}{\mathrm{d}x^2} + V(x)
\end{equation}
and the Schr\" odinger equation in \textit{imaginary time}, which is of the parabolic type, 
\begin{equation}
	\label{eq:imag_sch}
	\partial_t | \psi_t \rangle = - \hat{H} | \psi_t \rangle.
\end{equation}
In close analogy to arguments presented in the DMC section~\ref{subsubsec-PMC}, Kac noticed that the kinetic term of the Lagrangian in eq.~\eqref{eq:FPI} could be interpreted as a measure on Brownian walks, and a solution to the imaginary time Schr\" odinger equation can be written as
\begin{equation}
	\label{eq:fkac}
	\psi(x, t)=\underset{X \sim \text { Brownian with } X_{t}=x}{\mathbb{E}}
	\left[\exp \left(-\int_{0}^{t}  V\left(X_{\tau}, \tau \right) \mathrm{d}\tau \right) \psi\left(X_{0}, 0\right)\right],
\end{equation}
where only the \emph{endpoint} at time $t$ of the Brownian process is fixed, whereas the starting point at time $t=0$ is not. $\psi (x, 0)$ encodes the initial condition into this representation. When there is no external potential $V(x) = 0$, the Schr\" odinger equation in imaginary time is the diffusion equation and the Feynman-Kac solution is simply
\begin{equation}
	\begin{aligned} 
		\psi(x, t) &= \underset{X \sim \text { Brownian with } X_{t}=x}{\mathbb{E}}\left[\psi\left(X_{0}, 0\right)\right] \\
		&=  
		\frac{1}{\sqrt{2 \pi t}} \int  e^{-\left(x-x^{\prime}\right)^{2} / 2 t} \psi_{0}\left(x^{\prime}\right) \mathrm{d} x^{\prime}.
	\end{aligned}
\end{equation}
An illustration of the Feynman-Kac formula applied to a free particle in 1D is depicted in Fig.~\ref{fig:fk_1d_example}. The role of the potential $V(x)$ is to weight the Brownian paths, in turn defining the Feynman-Kac {path measure} $\mathbb{P}_{\mathrm{FK}}$ which is related to the Brownian measure $\mathbb{P}_{0}$ by the Radon-Nykodym derivative
\begin{equation}
\label{eq:RNderiv}
\frac{\mathrm{d} \mathbb{P}_{\mathrm{FK}}}{\mathrm{d} \mathbb{P}_{0}}=\mathcal{N} \exp \left(-\int V\left(X_{t}\right) d t\right),
\end{equation}
where $\mathcal{N}$ is a normalizing constant. Intuitively we can understand the measure as assigning more weight to Brownian paths that spend more time in the attractive region ($V(x) < 0$) than in repulsive regions ($V(x) > 0$). This is illustrated in Fig.~\ref{fig:fkac_measure_reweight}. 
\begin{figure}[H]
	\centering
	\subfloat
	{\includegraphics[width = \linewidth]{Chapter3/Figs/Raster/fkac_vs_fplanck_top.pdf}} \\
	\subfloat
	{\includegraphics[width = \linewidth]{Chapter3/Figs/Raster/fkac_vs_fplanck_mid1.pdf}} \\
	\subfloat
	{\includegraphics[width = \linewidth]{Chapter3/Figs/Raster/fkac_vs_fplanck_bottom.pdf}}
	
	\caption[Feynman-Kac for a free particle in 1D]{\textbf{Feynman-Kac for a free particle  in 1D.} $N=400$ Brownian walks starting from different $x_0$, the colour signifies initial position (\textbf{top}). In order to evaluate $\psi$ between $x-\frac{\delta x}{2}$ and $x+\frac{\delta x}{2}$ at some time $t$ we must first find Brownian paths that end there. The paths that pass through $x \in (1.5, 1.6)$ (blue) and through $x \in (-0.4,-0.3)$ (red) at $t=0.4$ are coloured, others are left in grey (\textbf{middle}). Time evolution of the initial condition $\psi_{0} = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^{2}}$, by estimating ${\mathbb{E}}\left[\psi\left(X_{0}, 0\right)\right]$ from the filtered paths at each timestep (\textbf{bottom}).}
	\label{fig:fk_1d_example}
\end{figure}
\begin{figure}[t]
	\centering
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Raster/reweight1.pdf}}
	\subfloat{\includegraphics[width=0.5\linewidth]{Chapter3/Figs/Raster/reweight2.png}}
	\caption[Feynman-Kac measure in a linear potential]{\textbf{Feynman-Kac measure in a linear potential.} 
		$N=30$ Brownian paths (\textbf{left}). The paths coloured ($P(\text{black})=1$, $P(\text{white})=0$) by their likelihood under the Feynman-Kac measure with $V(x)=x$ (\textbf{right}).}
	\label{fig:fkac_measure_reweight}
\end{figure}
Moreover, this new stochastic process is Markovian, meaning that a clear connection exists between the imaginary time Schr\" odinger equation and a SDE of form~\eqref{eq:SDE_general} with time-homogeneous $\sigma$ and $v$.
Indeed, in the continuous case the mapping between the Fokker-Planck equation~\eqref{eq:Fokker-Planck-General} and the Schr\" odinger equation exists in the form of a similarity transform. Starting from the FP equation of a stochastic process with constant volatility $\sigma = 1$
\begin{equation}
\label{eq:not_prime_cont_process}
\mathrm{d} X_{t}=\mathrm{d} W_{t}+v\left(X_{t}\right) \mathrm{d} t,
\end{equation} 
and drift $v(x)=-U^\prime(x)$ given as a gradient of some potential function $U(x)$, the pdf $\rho(t, x)$ of the process is
\begin{equation}
\frac{\partial \rho}{\partial t} = \frac{\partial}{\partial x}\left[\frac{\partial \rho}{\partial x}+U^{\prime}(x) \rho\right].
\end{equation}
We can define the function 
\begin{equation}
\psi(x, t)=\frac{\rho(x, t)}{\sqrt{\rho_{0}(x)}},
\end{equation}
with $\rho_{0}$ being the stationary distribution of the FP equation
\begin{equation}
\frac{\partial}{\partial x}\left[\frac{\partial \rho}{\partial x}+U^{\prime}(x) \rho\right] = 0 \quad \rightarrow \quad \rho_{0}(x) \propto \exp (-U(x)),
\end{equation}
which satisfies the imaginary time Schr\" odinger equation~\eqref{eq:imag_sch} with the Hamiltonian
\begin{equation}
\hat H=-\frac{\partial^{2}}{\partial x^{2}} \overbrace{-\frac{U^{\prime \prime}}{2}+\frac{U^{\prime 2}}{4}}^{\equiv V(x)}.
\end{equation}
The ground state of this Hamiltonian has zero energy and is 
\begin{equation}
\psi_{0}(x)=\sqrt{\rho_{0}(x)}.
\end{equation}
In other words, the quantum ground state probability distribution $|\psi_{0}|^2$ is the same as the classical stationary distribution $\rho_{0}$ of the stochastic process $X_{t}$ which is in the literature known as the {Nelson's ground state process}~\cite{nelson1967dynamical, albeverio1977energy}. This connection is one that our computational method will exploit, as the ability to efficiently sample from the stochastic process with correct drift $v$ is equivalent to sampling from the ground state of the quantum system. Even though the connection is simple, it comes with a caveat. Starting from the Schr\" odinger equation one needs to find the drift $v(x)$, and while the connection with $V(x)$ is clear-cut in this simple example, this is not the case in many-body systems. The \emph{inverse} problem of finding the stochastic process of a given Hamiltonian is difficult, and is one of the core problems approached in this thesis.

\subsection{Stoquastic Hamiltonians}
\label{subsec:fk-latt}
Before we illustrate the connection between the imaginary time Schr\" odinger equation and CTMCs we must introduce \textbf{Stoquastic Hamiltonians}~\cite{bravyi2006complexity}, a class of Hamiltonians which do not suffer from the sign problem.
\begin{definition}[Stoquastic Hamiltonian]
	A $k$-local Hamiltonian $\hat H=\sum_{i} \hat H_{i}$ is \textbf{stoquastic} if there exists a local basis $\mathcal{B}$ in which off-diagonal matrix elements of terms $\hat{H}_i$ are zero or negative
	\begin{equation}
		\langle x|\hat H| y\rangle \leq 0, \quad \forall x, y \in \mathcal{B} \text {\quad with } x \neq y.
	\end{equation}
\end{definition}
\noindent
If we consider the matrix $e^{-\tau \hat H}$ for a non-positive $\hat H$, we see that every term in the expansion
\begin{equation}
	e^{-\tau \hat{H}} = 1 -\tau \hat{H} + \frac{1}{2}(\tau \hat{H})^2 + \ldots
\end{equation}
is a non-negative matrix, thus so is $e^{-\tau \hat H}$. In the infinite time limit, the ground state is projected out
\begin{equation}
	\lim _{\tau \rightarrow \infty} e^{-\tau \hat H}=|\psi_0\rangle\langle\psi_0|, 
\end{equation}
and a global phase exists for which the ground state has non-negative amplitudes. Moreover, if $\hat H$ is irreducible then the ground state is node-less~\cite{discussion_stoquastic2017}
\begin{equation}
	\psi_0(x)>0 \text { for all } x \in \mathcal{B}.
\end{equation}
We can decompose a stoquastic Hamiltonian into a rate matrix $\Gamma$, defined according to section~\ref{subsubsect:CTMC}, and a diagonal potential matrix $V$
\begin{equation}
	\label{eq:hamilton_split}
	H = -\Gamma + V.
\end{equation}
The rates $\Gamma$ are analogous to the Brownian motion in the continuum case, and can be interpreted as the kinetic contribution. They represent a CTMC which we will understand as \emph{passive dynamics}. In terms of the Hamiltonian matrix,
\begin{equation}
\Gamma_{s \rightarrow s^{\prime}}=\left\{\begin{array}{ll}
-H_{s s^{\prime}} & \text { if } s \neq s \\
\sum_{s^{\prime} \neq s} H_{s s^{\prime}} & \text { if } s=s^{\prime}
\end{array}\right.
\end{equation}
and the potential is
\begin{equation}
V(\boldsymbol{s})=H_{s s}+\sum_{s^{\prime} \neq s} H_{s s^{\prime}}.
\end{equation}
From now on we use $\rightarrow$ notation to emphasise the transition between \textbf{adjacent states} $s\neq s^\prime$, which satisfy $H_{s s^{\prime}} \neq 0$. 

\subsection{Stoquastic representations of the lattice models}
\label{subsec:stoq_latt_forms}
Let us now reinterpret lattice models introduced in section~\ref{sec:lattice-models} in terms of the stoquastic Hamiltonian decomposition in eq.~\eqref{eq:hamilton_split}.

\subsubsection{Transverse-field Ising model}
\label{subsubsec:res-im}
In the $z$-spin basis a state is defined as $\mathbf{s} = \boldsymbol{\sigma}^z$, where $\boldsymbol{\sigma}^z = (\sigma_1^z, \sigma_2^z, \ldots, \sigma_N^z) \equiv (s_1^z, s_2^z, \ldots, s_N^z)$ and $\sigma_i^N$ represent either spin-up or spin-down state at site $i$. The TFIM Hamiltonian eq.~\eqref{eq:h-ising} in this representation becomes
\begin{equation}
	\langle \mathbf{s}^\prime | H_{\text{TFIM}} | \mathbf{s}\rangle = -J \sum_{\langle i, j \rangle} s_i s_j \langle \mathbf{s}^\prime | \mathbf{s}\rangle -h \sum_i \langle \mathbf{s}^\prime | \sigma_{i}^+ + \sigma_{i}^- | \mathbf{s} \rangle.
\end{equation}
If we examine the second term for $\mathbf{s}^\prime \neq \mathbf{s}$ we see that states will be adjacent, meaning there will be passive transitions between them, if the two configurations differ by only a single spin flip. In any such case the second term contributes $-h$ and the passive rates are
\begin{equation}
	\Gamma_{\mathbf{s}\rightarrow\mathbf{s}^\prime} = h.
\end{equation}
The passive process in the TFIM is then just individual spin flips, see Fig.~\ref{fig:isingpassive}. The potential follows from its definition
\begin{equation}
	V(\mathbf{s}) = -\sum_{\mathbf{s} \neq \mathbf{s}^\prime} \Gamma_{\mathbf{s}\rightarrow\mathbf{s}^\prime} + H_{ss} = -hN - J\sum_{\langle i, j \rangle} s_i s_j,
\end{equation}
where $N$ is the number of lattice sites.
\begin{figure}[h]
	\centering
	\includegraphics[height=4.5cm]{Chapter5/Figs/Vector/ising_passive}
	\caption[Ising passive process]{\textbf{Ising passive process.} Adjacent states in the TFIM are ones that differ by a single spin flip. The transition rates for all adjacent states are constant $h$ and the passive process flips individual spins according to the rates.}	
	\label{fig:isingpassive}
\end{figure}
\subsubsection{The XY model}
\label{subsubsec:res-hm}
In the $z$-spin basis the XY Hamiltonian~\eqref{eq:h-xy} becomes
\begin{equation}
	\langle \mathbf{s}^\prime | H_{\text{XY}} | \mathbf{s} \rangle = -\sum_{\langle i, j \rangle}\langle \mathbf{s}^\prime | \hat \sigma_i^+ \hat \sigma_j^- + \hat \sigma_i^- \hat \sigma_j^+| \mathbf{s}\rangle.
\end{equation}
Here two states $\mathbf{s} \neq \mathbf{s}^\prime$ are adjacent if we can find two neighbouring $i$, $j$ with $s_i \neq s_j$ in $\mathbf{s}$, such that swapping the values $s_i$ and $s_j$ gives $s^\prime$. The passive rates are then
\begin{equation}
	\Gamma_{\mathbf{s}\rightarrow\mathbf{s}^\prime} = 1,
\end{equation}
and the corresponding passive process is the \textbf{symmetric exclusion process} (SEP), depicted in Fig.~\ref{fig:xypassive}. The potential follows as
\begin{equation}
V(\mathbf{s}) = -\sum_{\mathbf{s} \neq \mathbf{s}^\prime} \Gamma_{\mathbf{s}\rightarrow\mathbf{s}^\prime} = -\sum_{\langle i, j \rangle} \left[n_i\left(1-n_j\right) - n_j\left(1-n_i\right)\right]
\end{equation}
\begin{figure}[t]
	\centering
	\includegraphics[height=4.5cm]{Chapter5/Figs/Vector/xy_passive}
	\caption[Symmetric exclusion process]{\textbf{Symmetric exclusion process.} Adjacent states of the XY model are ones separated by a switch of two non-equal neighboring spins, the associated passive process is the SEP \textbf{a)}. On subfigure $\mathbf{b)}$ a mask of all sites that can make a transition in the next time step (either up or right) is shown, the mask is necessary for implementing the SEP, see section~\ref{sec:qoptsampl}.}
	\label{fig:xypassive}
\end{figure}
where $n_i$ is the occupation number of site $i$.
%
%\subsubsection{XXZ model}
%\begin{equation}
%	\hat H_{\text{XXZ}} = \sum_{i, j} \left[\Delta \left(\hat \sigma_i^+ \hat \sigma_j^- + \hat \sigma_i^- \hat \sigma_j^+\right) + \hat{\sigma}^z_{i} \hat{\sigma}^z_{j} \right]
%\end{equation}

\subsection{Feynman-Kac in discrete state space}
The decomposition $H = -\Gamma + V$ allows us to define the Feynman-Kac formula in discrete state space~\cite{rogers2000diffusions} as
\begin{equation}
	\label{eq:fkac_disc}
	\psi\left(s_t, t\right)=
	\underset{\Sigma_{[0, t] \sim \Gamma}}{\mathbb{E}}
	\left[
	\exp \left(-\int_{0}^{t} V\left(\boldsymbol{s}_{t^{\prime}}\right) \mathrm{d} t^{\prime}\right) \psi\left(s_0, 0\right)
	\right].
\end{equation}
The expectation is now taken over the process driven by $\Gamma$ and weighted by the potential $V$. We denote the trajectory as $\Sigma_{[0, t]}$, where $\Sigma_{t^\prime}$ is the state of the system at time $t^\prime \in [0, t]$. Analogous as with the Feynman-Kac formula in continuous state space, this defines a new CTMC with measure $\mathbb{P}_{\text{FK}}$, which is related to  passive dynamics with measure $\mathbb{P}_0$ via the RN derivative, eq.~\eqref{eq:RNderiv}.

How exactly is this CTMC related to the imaginary time Schr\" odinger equation? Again via a similarity transform. The difference being that instead of Fokker-Planck we use the master equation to describe the time evolution of the pdf $P$
\begin{equation}
\frac{\partial P(s)}{\partial t}=\sum_{s^\prime \neq s}\left[\Gamma_{s^\prime \rightarrow s} P(s^\prime)-\Gamma_{s \rightarrow s^\prime} P(s)\right].
\end{equation}
The stationary state $P_0$ of the master equation satisfies detailed balance
\begin{equation}
	\Gamma_{k \rightarrow j}=\exp \left(\frac{V_{s^\prime}-V_{s}}{2}\right),
\end{equation}
and is thus
\begin{equation}
	P_{0}(s) \propto \exp \left(-V_{s}\right).
\end{equation}
The wave function 
\begin{equation}
	\psi(s, t)=\frac{P_{s}(t)}{\sqrt{P_{0}(s)}},
\end{equation}
then satisfies the imaginary time Schr\" odinger equation with the Hamiltonian
\begin{equation}
	\hat H_{s^\prime s}=\left\{\begin{array}{ll}
	-P_{0}^{-\frac{1}{2}}(s) \Gamma_{s^\prime \rightarrow s} P_{0}^{\frac{1}{2}}(s^\prime)=-1 & s^\prime \neq s \\
	\sum_{s^\prime \neq s} \Gamma_{s \rightarrow s^\prime} & s^\prime=s.
	\end{array}\right. 
\end{equation}
Again this Hamiltonian has a zero-energy ground state 
\begin{equation}
	\psi_{0}(s)=\sqrt{P_{0}(s)} \propto \exp \left(-\frac{V_{s}}{2}\right).
\end{equation}
The quantum probability in the ground state $|\psi_0(s)|^2$ coincides with the stationary distribution of the CTMC. The relation in the direction from Markov process to Hamiltonian is clear, but we are interested in the inverse, starting from the Hamiltonian and finding the corresponding stochastic process. We now turn our attention towards defining a suitable optimisation objective that, when optimised, will yield the correct Markov process.

%********************************** % New Section  *************************************
\section{Control theoretic approach to QM and loss functions}
\label{sec:control_loss}
\subsection{Holland Cost in continuous space}
In section~\ref{subsec:FK_in_continuous_space} we explored a connection between the ground state of some quantum system and It\^ o processes. We have seen that the ground state probability $|\psi_0|^2$ is also the stationary distribution $\pi$ of some stochastic process
\begin{equation}
	\label{eq:holl_opt_process}
	\mathrm{d} X_{t}=\mathrm{d} W_{t}+v^\prime\left(X_{t}\right) \mathrm{d} t,
\end{equation}
where $v^\prime$ is the optimal drift. Finding the correct drift in one-dimension was straightforward, but how to do it in higher dimensions ($W_{t}, X_{t} \in \mathbb{R}^n$ and $v: \mathbb{R}^n \rightarrow \mathbb{R}^n$) remained unanswered. Holland~\cite{holland1977cost} formulated the search for optimal drift as a stochastic control problem with cost function
\begin{equation}
	\label{eq:holland_cost}
	C[v]=\lim _{T \rightarrow \infty} \frac{1}{T} \mathbb{E}\left[\int \left(\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right)\mathrm{d} t\right],
\end{equation}
where the expectation is over the process in eq.~\eqref{eq:holl_opt_process}. Its derivation and proof of unique solution can be found in Appendix~\ref{app:holland-cost}. To see that the minimum of $C[v]$ really corresponds to the ground state, we first rewrite the cost in terms of the stationary distribution $\pi(x | v)$ of It\^ o process generated by the drift $v$
\begin{equation}
C[v]=\int \left[\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right] \pi(x \mid v)\mathrm{d}x.
\end{equation}
The distribution $\pi$ must satisfy the stationary Fokker-Planck equation
\begin{equation}
\frac{1}{2} \nabla^{2} \pi-\nabla \cdot(v \pi)=0,.
\end{equation}
Since the equality holds for drift $v = \frac{\nabla \psi}{\psi}$ and distribution $\pi = \psi^2$, cost is
\begin{equation}
C[v]=\int \left[\frac{1}{2}(\nabla \psi)^{2}+V\left(X_{t}\right) \psi^{2}\right]\mathrm{d}x.
\end{equation}
For normalised $\psi$ the integrand is the expected value of the quantum energy of the system, its minimum value $\lambda$ is the ground state energy $E_0$ which is achieved for the ground state wave function $\psi_{0}$. We could directly use the Holland's cost to find the optimal rates $v^\prime$, but we will instead use the fact that trajectories sampled under the Feynman-Kac measure $\mathbb{P}_{\mathrm{FK}}$ coincide with ones sampled from the optimal drift, and base our variational approach on finding the FK measure.

\subsection{Loss for continuous state space}
\label{subsec:continuous_loss}
To see that the path measures $\mathbb{P}_v$ of the process in eq.~\eqref{eq:not_prime_cont_process}  and the Feynman-Kac measure $\mathbb{P}_{\mathrm{FK}}$ coincide for the optimal rates $v^\prime$, we first need a measure of similarity between probability distributions. For this purpose we will use the \textbf{Kullback–Leibler} (KL) divergence $D_{\mathrm{KL}}$
\begin{equation}
	D_{\mathrm{KL}}(p \| q)=\underset{p}{\mathbb{E}}\left[\log \left(\frac{p}{q}\right)\right].
\end{equation}
The quantity is a divergence because of the asymmetry $D_{\mathrm{KL}}(p \| q) \neq D_{\mathrm{KL}}(q \| p)$. It is a strictly positive quantity $D_{\mathrm{KL}}(p \| q) \geq 0$ except for $p = q$ when $D_{\mathrm{KL}}(p \| q) = 0$. In order to obtain the KL divergence between $\mathbb{P}_v$ and $\mathbb{P}_{\mathrm{KL}}$ we first express the RN derivative of each path measure w.r.t. Brownian motion using Girsanov theorem~\eqref{eq:Girsanov_theorem}. Both RN derivatives can then be combined to express the logarithm Radon-Nikodym derivative or the \emph{log-likelihood} ratio as
\begin{equation}
\log \left(\frac{d \mathbb{P}_{v}}{d \mathbb{P}_{\mathrm{FK}}}\right)=
\tilde{\ell}-
E_{0} T+
\log 
\left(
\frac{\varphi_{0}\left(r_{0}\right)}{\varphi_{0}\left(r_{T}\right)}
\right),
\end{equation}
where we have defined $\tilde{\ell}$ as
\begin{equation}
\tilde{\ell} \equiv \int v\left(r_{t}\right) \mathrm{d} W_{t}+\int \left(\frac{1}{2}\left|v\left(r_{t}\right)\right|^{2}+V\left(r_{t}\right)\right)\mathrm{d} t.
\end{equation}
The boundary term depends the ground state $\varphi_0$ at initial $r_0$ and final $r_T$ points in the trajectory, and originates from the normalisation constant $\mathcal{N}$ in the Radon-Nikodym derivative. The KL divergence is 
\begin{equation}
\label{eq:ltilde-cont}
D_{\mathrm{KL}}\left(\mathbb{P}_{v} \| \mathbb{P}_{\mathrm{FK}}\right)=\underset{\mathbb{P}_{v}}{\mathbb{E}}\left[\ell^\prime-E_{0} T+\log \left(\frac{\varphi_{0}\left(r_{0}\right)}{\varphi_{0}\left(r_{T}\right)}\right)\right].
\end{equation}
If we consider the above Kullback-Leibler divergence in the long time limit $T \rightarrow \infty$, we see that the expectation of the first term of $\tilde{\ell}$ is zero. Moreover, if the marginal distributions $\psi_{0}(r_0)$ and $\psi_{0}(r_T)$ coincide, the boundary term vanishes as well, and the KL divergence is equivalent to the Holland cost, thus vanishes for optimal drift. This means that finding the correct path measure is equivalent to finding the optimal drift, and in standard machine learning fashion, minimizing $D_{\mathrm{KL}}$ can be used to find the optimal rates. Sampling the trajectories becomes a matter of integrating a SDE and can be done with some standard approach, e.g. Euler-Mayurama method. Tje gradients of $D_{\mathrm{KL}}$ w.r.t rate parameters $\theta$ can be obtained using stochastic backpropagation where the reparameterisation trick is used for each increment in the discretised SDE. The gradient of the boundary term is non-zero, but can be estimated by expressing $\psi_0$ in terms of $v_{\theta}$. Missing steps of the derivation can be found in Appendix~\ref{app:holland-prob}, and more details in~\cite{barr2020quantum}. 

\subsection{Todorov Cost in discrete state space}
\label{subsec:todorov}
We now turn our attention towards finding a variational approach in discrete state space and continuous time. We rely on foundational work done on linearly solvable Markov Decision Processes\footnote{For a general discussion of MDP's see~\cite{sutton2018reinforcement}} (MDP) by Todorov~\cite{todorov2007linearly, todorov2009efficient} and its applications to lattice problems~\cite{gispen2020ground}. The main idea is to reinterpret the dynamics of the imaginary time Schr\" odinger equation in the Todorov MDP framework by treating control as a modification of the passive dynamics of the system. The imaginary time Schr\" odinger equation can be written as
\begin{equation}
	\label{eq:sch_split}
	\frac{\partial \psi(s_j, t)}{\partial t}=-\underbrace{\sum_{s_k \neq s_j} \Gamma_{s_j \rightarrow s_k}\left[\psi(s_k, t)-\psi(s_j, t)\right]}_{\text{passive dynamics}}
	-V_{s_j} \psi(s_j, t)
\end{equation}
to emphasise the decomposition of the Hamiltonian eq.~\eqref{eq:hamilton_split} into passive dynamics and the potential. Akin to the transformation in discrete space, we use $\psi(s_j, t)=\exp[-u(s_j, t)]$, to arrive at an alternative form
\begin{equation}
	\label{eq:bellman_sch}
	-\frac{\partial u(s_j, t)}{\partial t}=\min _{\Gamma^{(v)}}\left[\ell(s_j, v)+\sum_{s_k} \Gamma_{s_j \rightarrow s_k}^{(v)}(u(s_k, t)-u(s_j, t))\right],
\end{equation}
where $\Gamma^{(v)}$ are the rates of the modified CTMC, and $\ell(s_j, v)$ is the cost per time associated with state $s_j$ and parameters $v$.
This is a form of Bellman equation, a very general concept in control theory and reinforcement learning, related to the Hamilton-Jacobi equation in physics. It is used to find the optimal actions, i.e. how we should choose $v$ at each moment w.r.t the cost associated with each state and parameters $\ell(s_j, v)$. The function $u(s_j, t)$ is the cost-to-go function, it is the cumulative cost obtained starting from state $s_j$ at time $t$ and acting optimally afterwards. The optimal parameters $v(t)$ are computed greedily with each step and guarantee minimum $u(s_j, t)$, this is most easily illustrated by looking $\Delta t$ into the past. Here the cost-to-go is given by the cost of remaining in the same state $\ell(s_j, t) \Delta t$ and the changes in costs-to-go when transitioning to other states weighted by the probability of making the transition $\Gamma^{(v)}_{s_j \rightarrow s_k}$
\begin{equation}
	u_{j}(t-\Delta t)=u_{j}(t)+\Delta t \min _{\Gamma^{(v)}}\left[\ell(j, v)+\sum_{k} \Gamma_{j \rightarrow k}^{(v)}(u(k, t)-u(j, t))\right].
\end{equation}
The cost $\ell(s_j, v)$ introduced by Todorov is of the form
\begin{equation}
	\ell(j, v)=q(j)+D_{\mathrm{KL}}(v(\cdot \mid j) \| p(\cdot \mid j)),
\end{equation}
where the first term encodes how undesirable different states are, and the second measures how costly the deviation from passive dynamics is, i.e. the agent (controlled rates) pay a price for reshaping the environment (passive rates). A more rigorous discussion can be found in Appendix~\ref{app:todorov-cost}. In the case of the imaginary time Schr\" odinger equation, these terms are
\begin{equation}
	q(s_j)=V(s_j) \Delta t,
\end{equation}
and
\begin{equation}
	D_{\mathrm{KL}}=\underset{\Sigma_{[0, t]} = k_t \sim \Gamma^{(v)}}{\mathbb{E}}\left[\sum_{n} D_{\mathrm{IS}}\left(\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}, \Gamma_{k^{(n)} \rightarrow k^{(n+1)}}\right)\right],
\end{equation}
where $\Sigma_{[0, t]}$ is the trajectory of states sampled from controlled dynamics $\Gamma^{(v)}$  which visits states $k^{(n)}$ for $t_{n-1} < t < t_{n}$ and $D_{\mathrm{IS}}$ is the Itakura-Saito divergence, see Appendix~\ref{app:todorov-cost}.

\subsection{Loss for discrete state space}
\label{subsec:discrete_loss}
Analogous to the derivation in continuous space, we will compare the cost to the Kullback-Liebler divergence between path measures $\mathbb{P}_v$ and $\mathbb{P}_{\mathrm{FK}}$ to see that the optimal rates $\Gamma^{(v)}$ coincide with sampling from the Feynman-Kac measure. Full derivations in~\ref{app:todorov-prob}.

The logarithm Radon-Nikodym between the controlled rates and Feynman-Kac measure is
\begin{equation}
	\label{eq:logrn-discrete}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{F K}}(k(t))\right)
	= \tilde \ell
	+\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)-E_{0} T-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right),
\end{equation}
where we have defined $\tilde \ell$ as
\begin{equation}
\tilde \ell = \int \left[V(k(t))+\sum_{l \neq k(t)}\left(\Gamma_{k(t) \rightarrow l}-\Gamma_{k(t) \rightarrow l}^{(v)}\right)\right] \mathrm{d}t.
\end{equation}
The KL divergence can be written as
\begin{equation}
	\begin{aligned}
		D_{\mathrm{KL}}\left(\mathbb{P}_{\nu} \mid \mathbb{P}_{\mathrm{FK}}\right) = \underset{P_{v}}{\mathbb{E}}\Bigg[\int V(k(t))&+\sum_{l \neq k(t)}\left(\Gamma_{k(t) \rightarrow l}-\Gamma_{k(t) \rightarrow l}^{(v)}\right)\\
		&+\Gamma_{k(t) \rightarrow l}^{(v)} \log \left(\frac{\Gamma_{k(t) \rightarrow l}^{(v)}}{\Gamma_{k(t) \rightarrow l}}\right) d t-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)\Bigg]-E_{0} T,
	\end{aligned}
\end{equation}
which is the Todorov's cost and vanishes for optimal rates, meaning that finding them is equivalent to finding the Feynman-Kac measured process. One might be tempted to use $D_{\mathrm{KL}}$ as an optimisation objective, but this is not possible because its gradient is biased due to the boundary term
$\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)$. Hence, we propose two alternatives.

\subsubsection{Loss no. 1: Variance of $\tilde \ell$}
The variance of quantity $\tilde{\ell}$
\begin{equation}
	\tilde{\ell}=
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{~d} \mathbb{P}_{F K}}(k(t))\right)
	-\sum_{n} \log \left(\frac{\Gamma^{(v)}_{k^{(n)} \rightarrow k^{(n+1)}}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)
	+E_{0} T
	+\log \left(\frac{\varphi\left(k^{(N)}\right)}{\varphi\left(k^{(0)}\right)}\right)
\end{equation}
 can be used as a loss function. Kolmogorov's criterion tells us that
 \begin{equation}
 	\log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)=\log \left(\frac{\varphi(k^{(n+1)})}{\varphi(k^{(n)})}\right),
 \end{equation}
 combined with the fact that the log Radon-Nikodym derivative is $0$ for correct transition rates $\Gamma^{(v)} = \Gamma^\prime$ means that $\operatorname{Var}[\tilde{\ell}]$ vanishes in that case, moreover it is bounded below by zero making it a suitable objective function. 
 
\subsubsection{Loss no. 2: Variance of log Radon-Nikodym derivative with fixed endpoints}
While the gradients of $D_{\mathrm{KL}}\left(\mathbb{P}_{\nu} \mid \mathbb{P}_{\mathrm{FK}}\right)$ are not accessible, we can still make use of the logarithm RN derivative in eq.~\eqref{eq:logrn-discrete}. It vanishes for optimal rates and while we can treat $E_0 T$ as constant, we have no easy way to approximate the wave function $\varphi$ in the boundary term which in general does not have zero variance. We can exploit the fact that log RN must be constant on all trajectories and consider a batch of trajectories with fixed endpoints. The variance of the boundary term vanishes if this is the case
\begin{equation}
\underset{\substack{\Sigma_{[0, t]} \sim r, \\ \text{with fixed } k^{(0)} \text{ and } k^{(N)}}}{\operatorname{Var}}\left[\tilde \ell
+\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)-E_{0} T-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)
\right] \underset{\Gamma^{(v)} = \Gamma^\prime}{\rightarrow} 0.
\end{equation}
The trajectories can be sampled using arbitrary rates $r$, but for convenience $\Gamma^{(v)}$ can be used. How to obtain a batch poses a technical challenge which is to be solved on a system by system basis, and is discussed in section~\ref{sec:qoptsampl}. This is the loss we focus on, we refer to it as the $\log \text{RN}$ loss. 
