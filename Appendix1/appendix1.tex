%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************

\chapter{Additional Derivations}

\section{Holland cost}
\label{app:holland-cost}
Following Holland's~\cite{holland1977cost} derivation of a stochastic control problem for the Schr\" odinger equation. We start by introducing the exponential transform
\begin{equation}
	\psi(x)=\exp [-U(x)],
\end{equation}
into the Schro\" odinger equation for $x \in \mathbb{R}^{n}$
\begin{equation}
	H \psi=\left[-\frac{1}{2} \nabla^{2}+V(x)\right] \psi=\lambda \psi,
\end{equation}
to obtain
\begin{equation}
	\label{eq:a-hc1}
	\frac{1}{2} \nabla^{2} U-\frac{1}{2}(\nabla U)^{2}+V(x)=\lambda.
\end{equation}
We can reinterpret the second term in eq.~\eqref{eq:a-hc1} as a minimum over all vectors $v$ for every $x \in \mathbb{R}^{n}$
\begin{equation}
	\label{eq:a-hc2}
	\frac{1}{2} \nabla^{2} U+\min_{v}\left[v \cdot \nabla U+\frac{1}{2}|v|^{2}+V(x)\right]=\lambda.
\end{equation}
We define $v$ to be a Lipshitz continuous function, i.e. the drift $v(x)$. Each drift now generates an It\^ o process
\begin{equation}
	\label{eq:a-hc3}
	\mathrm{d}X_{t}=\mathrm{d}W_{t}+v\left(X_{t}\right) \mathrm{d}t; \quad \text{ and } \quad X_0 = x,
\end{equation}
and we can define the cost function $C[v]$ for each $v$ as
\begin{equation}
	\label{eq:a-hc4}
	C[v]=\lim _{T \rightarrow \infty} \frac{1}{T} \mathbb{E}\left[\int \left(\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right)\mathrm{d}x \right].
\end{equation}
Holland~\cite{holland1977cost} proves the following theorem.
\begin{theorem}[Holland cost function]
	The minimum $\lambda = \min_{v} C[v]$, where the minimum is taken over drift functions $v: \Omega \rightarrow \mathbb{R}^n$, $\Omega \subset \mathbb{R}^n$ with Neumann boundary conditions $\frac{\partial \psi}{\partial n}=0 \text{ on } \partial \Omega$, is obtained only for $v=\frac{\nabla \psi}{\psi}=-\nabla U$.
\end{theorem}
\begin{proof}
	From eq.~\eqref{eq:a-hc2} follows the inequality
	\begin{equation}
		\label{eq:a-hc5}
		\frac{1}{2} \nabla^{2} U+v \cdot \nabla U+\frac{1}{2}|v(x)|^{2}+V(x) \geq \lambda,
	\end{equation}
	we notice that the first two terms are an infinitesimal generator $[\mathcal{L}_G U](x)$ of the process in eq.~\eqref{eq:a-hc3}, which is defined as
	\begin{equation}
		\left[\mathcal{L}_{\mathrm{G}} \psi\right](x) \equiv \lim _{t \rightarrow 0} \frac{\mathbb{E}_{x}\left[\psi\left(X_{t}\right)\right]-\psi(x)}{t},
	\end{equation} 
	where $\mathbb{E}_x$ is over processes with initial condition $X_0 = x$.
	With this in mind, we evaluate eq.~\eqref{eq:a-hc5} on the process $X_t$ and integrate over time to obtain
	\begin{equation}
		\frac{\mathbb{E}\left[U\left(X_{T}\right)-U(x)\right]}{T}+\mathbb{E}\left[\frac{1}{T}\int_{0}^{T} \left(\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right)\mathrm dt\right] \geq \lambda.
	\end{equation}
	In the $T \rightarrow \infty$ limit, the first terms disappears, so long as $U$ is bounded, and we are left with $C[v]$, meaning that we have proven the bound 
	\begin{equation}
		C[v] \geq \lambda.
	\end{equation}
	We see that the minimum $\lambda$ is achieved for control function
	\begin{equation}
		v = \frac{\nabla \psi}{\psi} = -\nabla U,
	\end{equation}
	in eq.~\eqref{eq:a-hc2}, the uniqueness of this optimal $v$ is due to the fact that if $v \neq \frac{\nabla \psi}{\psi}$ there cannot be a full equality in eq.~\eqref{eq:a-hc5}.
\end{proof}


\section{Probabilistic interpretation of Holland's cost function}
\label{app:holland-prob}
This section follows Barr et al.~\cite{barr2020quantum} which is closely related to previous work~\cite{dai1990markov}. To express the RN derivative we use the Girsanov theorem for both $\mathbb{P}_v$
\begin{equation}
	\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{0}}=\exp \left(\int v\left(X_{t}\right) \mathrm{d} X_{t}-\frac{1}{2} \int\left|v\left(X_{t}\right)\right|^{2} \mathrm{d} t\right),
\end{equation}
and $\mathbb{P}_{\mathrm{FK}}$
\begin{equation}
	\label{eq:a-hp2}
	\frac{\mathrm{d} \mathbb{P}_{\mathrm{FK}}}{\mathrm{d} \mathbb{P}_{0}}=\mathcal{N} \exp \left(-\int V\left(X_{t}\right) \mathrm{d} t\right).
\end{equation}
We combine both, and we have up to exponential accuracy $\mathcal{N} \sim e^{\lambda T}$
\begin{equation}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{\mathrm{FK}}}\right)=\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{0}} \frac{\mathrm{d} \mathbb{P}_{0}}{\mathrm{d} \mathbb{P}_{\mathrm{FK}}}\right) = \int v\left(X_{t}\right) \mathrm{d} X_{t}+\int \left(-\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right)\mathrm{d} t - \lambda T,
\end{equation}
finally we substitute $\mathrm{d} X_t = \mathrm{d}W_t + v(X_t)\mathrm{d}t$ to get
\begin{equation}
	\begin{aligned}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{\mathrm{FK}}}\right) 
	& =  \int v\left(X_{t}\right) \mathrm{d} W_{t} + \int |v(X_t)|^2 \mathrm{d}t + \int \left(-\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right)\mathrm{d} t\\
	& =  \int v\left(X_{t}\right) d W_{t}+\int \left(\frac{1}{2}\left|v\left(X_{t}\right)\right|^{2}+V\left(X_{t}\right)\right) \mathrm{d}t  - \lambda T.
	\end{aligned}
\end{equation}
A closer look at the normalisation constant $\mathcal{N}$ in eq.~\eqref{eq:a-hp2} gives rise to the boundary term. The normalisation is given by
\begin{equation}
	\mathcal N = \frac{\tilde{\psi}\left(r_{T}, T\right)}{\bar{\psi}\left(r_{0}, 0\right)},
\end{equation}
where $\tilde{\psi}\left(r_{t}, t\right)$ is the solution to the backwards imaginary time Schr\" odinger equation~\cite{barr2020quantum}, and is related to the distribution of the stochastic process in eq.~\eqref{eq:a-hc3} as
\begin{equation}
	\pi(r, t) = \tilde{\psi}\left(r, t\right) \psi\left(r, t\right).
\end{equation}
If we now take both distributions at initial time $\pi(r, 0)$ and terminal time $\pi(r, T)$ to be the ground state the normalisation constant becomes
\begin{equation}
	\frac{\tilde{\psi}\left(r_{T}, T\right)}{\bar{\psi}\left(r_{0}, 0\right)} = e^{E_{0} T} \frac{\varphi_{0}\left(r_{T}\right)}{\varphi_{0}\left(r_{0}\right)},
\end{equation}
and accounts for the boundary term and $E_0 T$ term in the Kullback-Leibler divergence.

\section{Todorov cost}
\label{app:todorov-cost}
Optimal decision processes are formalised using Markov Decision processes, here we follow work of Todorov~\cite{todorov2007linearly, todorov2009efficient} to fit the imaginary Schr\" odinger equation in this mould, we illustrate the concepts on the Ising model. An MDP is a $4$-tuple $\left(S, A, P_{a}, R_{a}\right)$:
\begin{itemize}
	\item $S$: Is the \emph{state space}, i.e. all possible configurations of the system $s_k$
	\item $U$: Is the \emph{control space}, i.e. all possible single-spin flips in each configuration
	\item $P_u$: Is the probability  $p(s^{(t+1)} = s^\prime | s^{(t)} = s, u^{(t)} = u)$ of control $u$ in state $s$ leading to state $s^\prime$ in the next time step
	\item $g$: Is the cost received when moving from state $s$ to $s^\prime$ due to $u$
\end{itemize}
The optimal decision/control problem is then given by the Bellman equation for the optimal cost-to-go function $v(s)$
\begin{equation}
	\nu(s)=\min _{u}\left\{
	\underbrace{\ell(s, u)}_{\text{immediate cost}}
	+
	\underbrace{\mathbb{E}_{s^{\prime} \sim p(\cdot \mid s, u)}\left[\nu\left(x^{\prime}\right)\right]}_{\text{expected cost of next state}}
	\right\}.
\end{equation}
Todorov introduces a formalism where the agent does not perform specific symbolic actions (e.g. flips a certain spin) but is instead allowed to specify transition probabilities $u(s^\prime|s)$. Formally this means 
\begin{equation}
	p\left(s^{\prime} \mid s, u\right)=u\left(s^{\prime} \mid s\right),
\end{equation}
the agent reshapes the dynamics of the system as it wishes, but for this it pays a price depending on how much it changes the dynamics. In absence of controls $u$ the system follows \emph{passive dynamics} $p(s^\prime | s)$ which correspond to the first term in eq.~\eqref{eq:hamilton_split} of the stoquastic Hamiltonian. The cost is thus
\begin{equation}
	\ell(s, u)=\underbrace{q(s)}_{\text{state cost}}+\underbrace{D_\mathrm{KL}((\cdot \mid s) \| p(\cdot \mid s))}_{\text{control cost}}.
\end{equation}
Optimal control problem in this form can be linearised in terms of the \emph{desirability} function $z(s, t)=\exp (-\nu(s, t))$, yielding optimal dynamics $\nu^\prime$
\begin{equation}
	\nu^{\prime}(s_j \mid s_k)=\frac{p(s_j \mid s_k) z(s_j)}{\sum_{s_l} p(s_l \mid s_k) z(s_l)},
\end{equation}
with
\begin{equation}
	\label{eq:lintod}
	z(s_k, t)=e^{-q(s_k)} \sum_{s_j} p(s_j \mid s_k) z(s_j, t+1)
\end{equation}
and it is this linear equation, what we can connect to the imaginary time Schr\" odinger equation. We start by transforming the MDP into continuous time (transition probabilities to rates $p, u \rightarrow \Gamma, \Gamma^{(v)}$) as
\begin{equation}
	p(s_j \mid s_k)=\left\{\begin{array}{ll}
	1-\Delta t \sum_{s_l} \Gamma_{s_k \rightarrow s_l} & s_j=s_k \\
	\Delta t \Gamma_{s_k \rightarrow s_j} & s_j \neq s_k
	\end{array}\right.
	, \quad \text{and} \quad 
	u(s_j \mid s_k)=\left\{\begin{array}{ll}
	1-\Delta t \sum_{s_l} \Gamma_{s_k \rightarrow s_l}^{(v)} & s_j=s_k \\
	\Delta t \Gamma_{s_k \rightarrow s_j}^{(v)} & s_j \neq s_k
	\end{array}\right.
\end{equation}
and setting $q(s_j) = \Delta t V(s_j)$ eq.~\eqref{eq:lintod} becomes
\begin{equation}
	\begin{aligned}
	z(s_k, t) &= e^{-\Delta t V(s_k)} \left[
				\underbrace{z(s_k, t + \Delta t) - \Delta t \sum_{s_l \neq s_k}\Gamma_{s_{k} \rightarrow s_{l}} z(s_k, t + \Delta t)}_{\text{from } s_j = s_k}
				+
				 \underbrace{\Delta t \sum_{s_j \neq s_k} \Gamma_{s_k \rightarrow s_j} z(s_j, t + \Delta t)}_{\text{from } s_j \neq s_k}
				\right] \\
			  &= \left[1 - \Delta t V(s_k) + \ldots\right]\cdot
			     \left[z(s_k, t + \Delta t) 
			     - \sum_{s_j \neq s_k} \Gamma_{s_k \rightarrow s_j} 
			     	\left[
			     	z(s_j, t + \Delta t) - z(s_k, t + \Delta t)\right]
		     	\right]
	\end{aligned}
\end{equation}
keeping only the first order in $\Delta t$ and dropping the unnecessary $\neq$ in the sum
\begin{equation}
	\frac{z(s_k, t) - z(s_k, t + \Delta t)}{\Delta t} = V(s_k) z(s_k, t + \Delta t) - \sum_{s_j} \Gamma_{s_k \rightarrow s_j} 
	\left[
	z(s_j, t + \Delta t) - z(s_k, t + \Delta t)\right]
\end{equation}
taking limit $\Delta t \rightarrow 0$ finally gives
\begin{equation}
	-\frac{\mathrm{d} z(s_k, t)}{\mathrm{d} t} =  V(s_k) z(s_k, t) - \sum_{s_j} \Gamma_{s_k \rightarrow s_j} 
	\left[
	z(s_j, t) - z(s_k, t)\right],
\end{equation}
it is the imaginary time Schr\" odinger equation~\eqref{eq:sch_split}.

\noindent
The $D_{\mathrm{KL}}$ in the loss $\ell (s, v)$ is expressed in the same manner
\begin{equation}
	\begin{aligned}
		D_{\mathrm{KL}}(v(\cdot \mid s_k) \| p(\cdot \mid s_k))
		&=\left[1-\Delta t \sum_{s_l} \Gamma_{s_k \rightarrow s_l}^{(v)}\right] \log \left[\frac{1-\Delta t \sum_{s_l} \Gamma_{s_k \rightarrow s_l}^{(v)}}{1-\Delta t \sum_{s_l} \Gamma_{s_k \rightarrow s_l}}\right]+\Delta t \sum_{s_j \neq s_k} \Gamma_{s_k \rightarrow s_j}^{(v)} \log \left[\frac{\Gamma_{s_k \rightarrow s_j}^{(v)}}{\Gamma_{s_k \rightarrow s_j}}\right]\\
		&= \Delta t \sum_{s_j \neq s_k} \Gamma_{s_k \rightarrow s_j}^{(v)}\underbrace{\left(\log \left[\frac{\Gamma_{s_k \rightarrow s_j}^{(v)}}{\Gamma_{s_k \rightarrow s_j}}\right]+\frac{\Gamma_{s_k \rightarrow s_j}}{\Gamma_{s_k \rightarrow s_j}^{(v)}}-1\right)}_{\mathrm{Itakura-Saito~divergence~} D_{\mathrm{IS}}(\Gamma_{s_k \rightarrow s_j}^{(v)}, \Gamma_{s_k \rightarrow s_j})},
	\end{aligned}
\end{equation}
If we consider an ensemble of systems in state $s_k$ there is $\Delta t \Gamma_{s_k \rightarrow s_j}^{(v)}$ probability of transitioning $s_k \rightarrow s_j$ in the next time increment, meaning that the ensemble contribution to the $D_{\mathrm{KL}}$ each time increment is $\sum_{s_j} \Delta t \Gamma_{s_k \rightarrow s_j}^{(v)} D_{\mathrm{IS}}\left(\Gamma_{s_k \rightarrow s_j}^{(v)}, \Gamma_{s_k \rightarrow s_j}\right)$, thus we can express the Kullback-Liebler divergence as
\begin{equation}
	D_{\mathrm{KL}}=\underset{\Sigma_{[0, t]}=k_{t} \sim \Gamma^{(v)}}{\mathbb{E}}\left[\sum_{n} D_{\mathrm{IS}}\left(\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}, \Gamma_{k^{(n)} \rightarrow k^{(n+1)}}\right)\right].
\end{equation}
\section{Probabilistic interpretation of Todorov's cost function}
\label{app:todorov-prob}
To find the Radon-Nikodym derivative between $\mathbb{P}_v$ and $\mathbb{P}_{\mathrm{FK}}$ we proceed in the same way as in the continuous case, by first finding the respective RN derivatives with the passive process. From the discrete space Feynman-Kac formula~\eqref{eq:fkac_disc} follows
\begin{equation}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{0}}{\mathrm{d} \mathbb{P}_{F K}}(k(t))\right)=\int V(k(t)) d t-E_{0} T-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right),
\end{equation}
and by using the Girsanov theorem equivalent for discrete space\footnote{see proposition 2.6 in Appendix 1 of~\cite{kipnis1998scaling}} we obtain
\begin{equation}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{d} \mathbb{P}_{0}}(k(t))\right)
	=
	\int \sum_{l \neq k(t)}\left(\Gamma_{k(t) \rightarrow l}-\Gamma_{k(t) \rightarrow l}^{(v)}\right) d t+\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right).
\end{equation}
We combine both
\begin{equation}
	\begin{aligned}
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{~d} \mathbb{P}_{\mathrm{FK}}}\right)
	=
	\log \left(\frac{\mathrm{d} \mathbb{P}_{v}}{\mathrm{~d} \mathbb{P}_{0}} \frac{\mathrm{d} \mathbb{P}_{0}}{\mathrm{~d} \mathbb{P}_{\mathrm{FK}}}\right)
	= \tilde \ell
	+\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)-E_{0} T-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right),
	\end{aligned}
\end{equation}
with
\begin{equation}
	\tilde \ell = \int \left[V(k(t))+\sum_{l \neq k(t)}\left(\Gamma_{k(t) \rightarrow l}-\Gamma_{k(t) \rightarrow l}^{(v)}\right)\right] \mathrm{d}t.
\end{equation}
To see that zero $D_{K L}\left(\mathbb{P}_{v} \| \mathbb{P}_{\mathrm{F K}}\right)$ coincides with rates that minimize Todorov's cost, we need
\begin{equation}
	\underset{\mathbb{P}_v}{\mathbb{E}}
	\left[
	\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)
	\right]
	= 
	\underset{\mathbb{P}_v}{\mathbb{E}} 
	\left[
	\int 
	\sum_{l \neq k(t)}
	\Gamma_{k(t) \rightarrow l}^{(v)} \log \left(\frac{\Gamma_{k(t) \rightarrow l}^{(v)}}{\Gamma_{k(t) \rightarrow l}}\right) \mathrm{d} t
	\right],
\end{equation}
which holds because the expectation of a contribution of a single step of the trajectory $k^{(n)} \rightarrow k^{(n+1)}$ is equivalent to an ensemble average starting from the same state weighted by the probability of jump $\Gamma_{k(t) \rightarrow l}^{(v)} \Delta t$, this holds separately for each step in the trajectory and by writing $\sum_{t_i} \cdots~\Delta t_i \rightarrow \int \cdots~\mathrm{d}t$ we obtain above equality. The KL divergence then becomes
\begin{equation}
	\begin{aligned}
		D_{\mathrm{KL}}\left(\mathbb{P}_{v} \| \mathbb{P}_{\mathrm{FK}}\right) = 
		\underset{\mathbb{P}_{v}}{\mathbb{E}}\Bigg[\int V(k(t))+\sum_{l \neq k(t)}\Big(\Gamma_{k(t) \rightarrow l}&-\Gamma_{k(t) \rightarrow l}^{(v)}\Big) \mathrm{d} t +\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right) \\
		&-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)\Bigg]-E_{0} T 
	\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
D_{\mathrm{KL}}\left(\mathbb{P}_{v} \| \mathbb{P}_{\mathrm{FK}}\right) = \underset{\mathbb{P}_{v}}{\mathbb{E}}\Bigg[\int V(k(t))+\sum_{l \neq k(t)}\big(\Gamma_{k(t) \rightarrow l}&-\Gamma_{k(t) \rightarrow l}^{(v)}\big)+\Gamma_{k(t) \rightarrow l}^{(v)} \log \left(\frac{\Gamma_{k(t) \rightarrow l}^{(v)}}{\Gamma_{k(t) \rightarrow l}}\right) \mathrm{d} t\\
&-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)\Bigg]-E_{0} T,
\end{aligned}
\end{equation}
which agrees with Todorov.