%!TEX root = ../thesis.tex
% **************************** Define Graphics Path **************************
\ifpdf
\graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
\graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

%*******************************************************************************
%****************************** Fifth Chapter *********************************
%*******************************************************************************

\chapter{Experiments and Results}
\label{chapter5}
In the penultimate chapter we present results obtained from the methodology and software presented in Chapter~\ref{chapter4}. We begin by discussing the training of rates $\Gamma^{(v)}$ and problems that we run into, before turning our attention to sampling with the obtained rates.  

\section{Training the rates}
\label{sec:training_exp}

\subsection{The elusive timescale $\lambda$}
\label{subsec:elusive_lambda}
An intuitive explanation of the Todorov loss introduced in section~\ref{subsec:todorov}, is in terms of penalisation for deviations away from the passive dynamics of the system, i.e. an agent has full control over the system but pays for straying far from the passive dynamics. Our method in its most basic form, using the \emph{latest} variational rates $\Gamma^{(v)}_{s \rightarrow s^\prime}$ to sample and using \emph{permute} batch generation, seems to completely disregard this penalty for straying far away from the passive dynamics when applied to the TFIM. Optimised rates $\Gamma_*^{(v)}$ obtained with this method vary depending on the initialisation, but the training itself is not sensitive to it. It is robust in the sense that low loss is obtained irrespective of the initialisation, barring vary bad hyperparameter settings, but the rates $\Gamma_*^{(v_i)}$ corresponding to these low losses are not the same. At first glance it appears that there are many \emph{local optima} for the rates, or alternatively, that there is an invariance in the loss and many rates are optimal. The latter turns out to be true. We refer to this problem as the inability to learn the correct \textbf{timescale} $\lambda$. This issue is illustrated in Fig.~\ref{fig:scaleblowup} which shows the number of spin flips in the sampled trajectory throughout training for three initialisations, along with the distributions of holding times $P(\tau) \sim \lambda e^{-\lambda x}$ of the final learned rates.
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Vector/scale_blowup}
	\caption[The inability of the method to learn the correct timescale $\lambda$]{\textbf{The inability of the method to learn the correct timescale $\lambda$.} The $\log \text{RN}$ + \emph{permute} batch, fails to learn the correct timescale of the TFIM model. Figure shows the discrepancy between three learned rates in 1D ($L=8, J=1.0, g=1.0$). We see that the number of flips in the original sampled trajectory varies wildly throughout training, and is not related to the loss (\textbf{top}). All three initialisations reach low loss, but corresponding learned rates vary in scale, as exemplified by the distribution of holding times $\tau_i$ of each CTMC (\textbf{bottom}). The rate parameter $\lambda$ can be orders of magnitude different from the passive rates $\lambda_0$ (black dashed line).}
	\label{fig:scaleblowup}
\end{figure}
If we assume that the holding times are approximately exponentially distributed\footnote{In each state the holding time is exponentially distributed, see section~\ref{subsubsect:CTMC}.} and compare the rate constants $\lambda$ for different optimal rates, we see that they can be orders of magnitude different. Yet the obtained rates do appear to have something in common, the similarity can be seen in Fig.~\ref{fig:rate_structure}, which depicts a pair of optimized rates for 1d-TFIM ($L=6$) and the absolute difference of their \emph{jump chains}. The obtained rates seem to share jump chains $T^{(v)}_{s_i \rightarrow -s_i}$ and differ in holding time distributions. It is important to note, that this jump chain is not necessarily the correct jump chain, but it merely smooths out the $\log \text{RN}$ values in this type of batch to achieve low loss. Why is this the case?
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Vector/rate_structure.pdf}
	\caption[Structure of learned rates for 1d-TFIM]{\textbf{Structure of rates in 1d-TFIM.} Figure displays the underlying \emph{structure} learned in the 1d-TFIM ($L=6, J=1.0, h=1.0$) using the simple permutation batch. The optimal rates $\Gamma^{(v_1)}_{s_i \rightarrow -s_i}$, $\Gamma^{(v_2)}_{s_i \rightarrow -s_i}$ obtained with two initialisations (\textbf{middle}) for each configuration (\textbf{top}), look very similar. The jump matrices $T^{(v_1)}_{s_i \rightarrow -s_i}$, $T^{(v_2)}_{s_i \rightarrow -s_i}$ corresponding to the learned CTMCs are nearly the same (\textbf{bottom}), with maximum absolute difference being $\Delta_{\text{max}} T_{s_i \rightarrow -s_i} \approx 2\cdot10^{-3}$.}
	\label{fig:rate_structure}
\end{figure}

The issue in learning the timescale is not in the logarithm Radon-Nikodym loss, but in the way we generate batches. Moreover, this issue applies to a wider range of models than just the TFIM. For any model which has the following properties:
\begin{enumerate}
	\item The passive rates are constant and the same for all adjacent states $\Gamma_{s \rightarrow s^{\prime}}=g$
	\item The number of adjacent states $N_a$ is independent for any state $s$
\end{enumerate}
it the $\log \text{RN}$ loss is \textbf{detached} from the passive rates $g$, if the variance is calculated from a batch of trajectories with same time $T$ and number of actions $N_{\text{actions}}$. Detached in this context means that the passive rates $g$ have no effect on the variance and hence on the optimisation of the rates. This can be seen by inserting 
\begin{equation}
	V(\mathrm{s})=-\sum_{s \neq s^{\prime}} \Gamma_{s \rightarrow s^{\prime}}+H_{s s}=-g N_a + H_{s s} \quad \text{ and } \quad \Gamma_{s \rightarrow s^{\prime}}=g
\end{equation}
into
\begin{equation}
	\begin{aligned}
			{\operatorname{Var}}\Bigg[\int\bigg(V(k(t))+&\sum_{l \neq k(t)}\bigg(\Gamma_{k(t) \rightarrow l}-\Gamma_{k(t) \rightarrow l}^{(v)}\bigg)\bigg) \mathrm{d} t \\
		&+\sum_{n} \log \left(\frac{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}}{\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}}\right)-E_{0} T-\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right)
		\Bigg],
	\end{aligned}
\end{equation}
to obtain
\begin{equation}
	\begin{aligned}
		\operatorname{Var}\Bigg[\int\left(H_{s s}-\sum_{l \neq k(t)}\Gamma_{k(t) \rightarrow l}^{(v)}\right)\mathrm{d} t 
		&+ \sum_{n} \log \left(\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}\right)\Bigg] \\ &-E_{0} T -\log \left(\frac{\varphi(k^{(N)})}{\varphi(k^{(0)})}\right) \textcolor{red}{- gN_aT + gN_aT - N_{\text{actions}}\log{(g)}}.
	\end{aligned}
\end{equation}
The terms in red contain the passive rates, and can be taken out of $\operatorname{Var}[\cdot]$. The TFIM obeys both listed properties, and a \emph{permute} batch has constant $T$ and $N_{\text{actions}}$, meaning that we find ourselves in a regime where the correct timescale $\lambda$ \textbf{cannot} be learned. 

\subsection{Alternative Batch generation}
Unlearnable timescale for the \emph{permute} batch puts us in a precarious situation. The only way forward in models with the timescale learning issue is to work with trajectories of different lengths $N_{\text{actions}}$, fixed endpoints, and fixed time\footnote{Otherwise we cannot neglect $E_0T$ term.} $T$. This is computationally disadvantageous, because we can no longer evaluate $\log \text{RN}$ on the whole batch in parallel. Moreover, the generation of the batch becomes difficult to implement and may become a bottleneck itself. 

We test the \emph{construct} batch, an alternative batch generation technique described in section~\ref{sec:qoptsampl}. 
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Vector/constr-g-loss}
	\caption[Rate training in 1d-TFIM using the \emph{construct} method]{\textbf{Rate training in 1d-TFIM using the construct method}. Optimisation with the \emph{construct} batch method, converges for different values of parameter $h$, moreover the obtained rates are not independent of $h$. Solved with $L=6, N_a=4, N_e=2, T=1$.}
	\label{fig:constr-g-loss}
\end{figure}
Fig.~\ref{fig:constr-g-loss} displays the training loss for the TFIM with different values of $h$ when using the \emph{construct} method for batch generation. Two things are apparent when optimising this new loss. First, it is even more volatile and increasing the batch size helps less than with the \emph{permute} batch. Second, the optimisation becomes more difficult when $h$ is smaller. It is also clear, that this loss now has the capability to scale the outputs of the model throughout optimisation, and at least for small times $T$ the timescale of the model is close to the passive timescale $\lambda_0$. This is clearly seen in Fig.~\ref{fig:troub}, where distributions of holding times are shown for a range of optimisation settings. 
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{Chapter5/Figs/Vector/troub}
	\caption[Distributions of holding times $\tau$ in the 1d-TFIM with \emph{construct} batch]{\textbf{Distributions of holding times $\tau$ in the 1d-TFIM with \emph{construct} batch.} The $\log\text{RN}$ loss is no longer decoupled from the rates, we see that the timescale $\lambda$ is learned by the model and is comparable to the passive rates $\lambda_0=hL$ for range of setups ($T$, $h$, at $L=6$).}
	\label{fig:troub}
\end{figure}
\noindent
However, taking a look at the optimised rates for a range of values of $h$ is worrying, shown in Fig.~\ref{fig:troubrates}. The structure of the rates does not change much when varying parameter $h$. Even though the system undergoes a phase transition between $h=0.4$ and $h=1.6$ this is not evident from the obtained transition probabilities. Moreover, while the optimal rates scale correctly with $h$, they are nearly uniform, meaning that the distribution has minimal preference about which spin to flip, regardless of $h$ and state. This is also exemplified when using the rates to perform sampling, the results are remarkably similar to using passive rates, see~\ref{subsec:impsampl}. 
\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{Chapter5/Figs/Vector/troub_rates}
	\caption[Learned rates of the 1-dTFIM with the \emph{construct} batch]{\textbf{Learned rates of the 1-dTFIM with the \emph{construct} batch.} The optimised rates for the 1d-TFIM scale with $h$, but are nearly uniform. }
	\label{fig:troubrates}
\end{figure}

For a comparison of jump chains $T$ that belong to the optimised rates at different $h$, refer to Fig.~\ref{fig:ratecompare2}. And for a comparison between jump chains obtained with different batch construction methods, see Fig.~\ref{fig:ratecompare1}. While the absolute difference between jump matrices in either of the two cases may seem small, this does not speak to any hidden structure uncovered by the loss, as it is merely a consequence of the jump matrices being nearly uniform. Most jump probabilities fall in the range $T_{i \rightarrow j} \in (0.16, 0.17)$ which is remarkably close to $\frac{1}{6} \approx 1.66$.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\linewidth]{Chapter5/Figs/Vector/Tvlamb}
	\caption[Timescale gap for the \emph{construct} batch.]{\textbf{Timescale gap for the \emph{construct} batch.} When optimising the rates $\Gamma^{(v)}$ using the $\log \text{RN}$ loss and \emph{construct} batch, we find ourselves in one of two learning regimes depending on the simulation time $T$ at constant $h$ and constant range ($N_a^{(\text{min})}$, $N_a^{(\text{max})}$). For small times $T$ the rates are forced towards the passive rates but after a certain $T_\text{treshold}$ the timescale has a hard time converging. Figure shows example for $h=0.6$ and $N_a^{(\text{max}-\text{min})} = 60$.}
	\label{fig:tvlamb}
\end{figure}
% T and h interplay
While training the rates using the \emph{construct} batch method with different parameters $T$, $h$ and $N_b$, it is qualitatively evident that we may find ourselves in one of two training regimes. We have either quick convergence with optimised rates very close to the passive dynamics, such as in Fig.~\ref{fig:constr-g-loss}, or we have trouble converging at all and the optimised rates stray far away from the passive dynamics. Neither of the two regimes leads to the right solution. The "phase transition" from one learning regime to another occurs when we increase the simulation time $T$ and do not change $N_b$ or $h$, see Fig.~\ref{fig:tvlamb}. Let us examine the loss again,
\begin{equation}
\begin{aligned}
\operatorname{Var}\Bigg[\int\left(H_{s s}-\sum_{l \neq k(t)}\Gamma_{k(t) \rightarrow l}^{(v)}\right)\mathrm{d} t 
+ \sum_{n} \log \left(\Gamma_{k^{(n)} \rightarrow k^{(n+1)}}^{(v)}\right) \textcolor{red}{- N_{\text{actions}}\log{(g)}}\Bigg],
\end{aligned}
\end{equation}
where we have purposely left out the terms that cancel out or are constant over all trajectories in the batch. The third term, is the only term tying the variance to the passive rates. In any batch there will be two trajectories with the maximum difference in length $N_a^{(\text{max}-\text{min})}$. This puts a bound on how much the third term can vary across trajectories in a batch. If we increase the time $T$, the variability of first term grows proportionally, while it stays constant for the other two terms. This means that at a certain time $T$ the majority contribution will be from the first term. We find ourselves in a similar situation to using the \emph{permute} batch, where the passive terms do not matter at all and the timescale is impossible to learn\footnote{Although this case is not as extreme.}. Unfortunately this same argument can be applied to the \emph{split} batch, which may be limited in this same way.

A way around this may be to have $N_a^{(\text{max}-\text{min})}$ vary from epoch to epoch, but this introduces additional computational cost, as well as additional stochasticity into the loss. Varying the time $T$ for each epoch would be easier to implement, since the holding times are arbitrarily assigned to states anyway. 

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Chapter5/Figs/Vector/rate_compare1}
	\caption[Comparison of \emph{jump matrices} $T$ obtained with different batch types]{\textbf{Comparison of \emph{jump matrices} $T$ obtained with different batch types.}}
	\label{fig:ratecompare1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Chapter5/Figs/Vector/rate_compare2}
	\caption[Comparison of \emph{jump matrices} $T$ for different batch types]{\textbf{Comparison of \emph{jump matrices} $T$ for different $h$.}}
	\label{fig:ratecompare2}
\end{figure}


\newpage
\subsection{Architectural choices}
\subsubsection{The pCNN}
As expected architecture choices and hyperparameters play an important role in learning the rates $\Gamma^{(v)}$, however how each separate parameter impacts the optimisation process is not very surprising. More interesting is perhaps the relative importance we can assign to each hyperparameter when compared to others. As we will see hyperparameters of the pCNN play one of two roles. The CTMC simulation time $T$ and batch size $N_b$ stabilise the estimation of variance loss, thus improving the stability of the learning process and convergence, while the width $N_w$ of the pCNN and its depth $N_l$ increase the representational power of the NN.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Vector/init_test_learning}
	\caption[Initial rate training experiments]{\textbf{Initial rate training experiments}. Learning experiments using \emph{initial} sampling mode and different simulation setups for 2d-TFIM ($L=3, J=1.0, h=1.0$). \textbf{top left:} $T = 75, N_w = 5, N_l = 3$, \textbf{top right:} $N_b = 50, N_w = 5, N_l = 3$, \textbf{bottom left:} $N_b = 50, T = 50, N_l = 3$, \textbf{bottom right:} $N_b = 50, T = 50, N_w = 5$.}
	\label{fig:inittestlearning}
\end{figure}
Initial experiments are shown in Fig.~\ref{fig:inittestlearning}, the top row displays the aforementioned stability. Increasing the batch size decreases the "volatility" of the loss, and same is true for $T$, but only if we consider loss normalized per time unit. As a rule of thumb $N_w$ and $N_l$ do not have a significant impact on the efficiency and speed of training, so long as both hyperparameters are chosen to be somewhere in the goldilocks zone, deep and narrow networks sometimes struggle to converge, (bottom right) Fig.~\ref{fig:inittestlearning}.

Going beyond the training itself, we need to evaluate the \emph{generalisation} of the rates. The worry is that training the rates at certain $T$ and $N_b$ could overfit and negatively impact sampling down the line, where the $T$ will be very different. To test this the rates were trained for various settings of $T, N_b, N_w, N_l$ and tested them by evaluating the loss at $T=50$, $N_b=50$, Fig.~\ref{fig:avgvarloss}. What we learn is that the rates do not overfit, average test loss is in the same order of magnitude than the train loss, and that $N_b$ is more important for stability than $T$. Finally, as we want to construct the most effective ansatz for eventual sampling experiments, we also investigate the relative time and size cost of increasing hyperparameters, the results are in Fig.~\ref{fig:initialtimespace}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Raster/avg_var_loss}
	\caption[Generalisation capabilities of the pCNN]{\textbf{Generalisation capabilities of the pCNN.} Figure displays the effect of $T, N_b$ and $N_w, N_l$ on transferability of the learned rates using initial sampling, 2d-TFIM (L=3, J=1.0, h=1.0). Data was obtained by training the rates for different setups, sampling $N=15$ trajectories with $T=50$ from the learned rates for each setup, permuting the trajectories to obtain batches of $N_b=50$ and finally evaluating the average and sample variance of the $\log \text{RN}$ loss. Subfigures show (left to right) the loss after final epoch of training, average test loss, and variance of test loss. Setups: varying $N_b$ and $T$ at $N_w=3$, $N_l=3$ (\textbf{top}), varying $N_w$ and $N_l$ at $N_b=32$ and $T=30$ (\textbf{bottom}). All loss values are $T$ normalised.}
	\label{fig:avgvarloss}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Chapter5/Figs/Raster/initial_time_space}
	\caption[Time and space complexity of the pCNN]{\textbf{Time and space complexity of the pCNN.} Comparison of normalised time needed per epoch averaged over $N=15$ samples, for different $T$, $N_b$ (\textbf{top left}) and $N_w$, $N_l$ (\textbf{top right}). Comparison of normalised batch size averaged over $N=15$ samples, for different $T$, $N_b$ (\textbf{bottom left}) and $N_w$, $N_l$ (\textbf{bottom right}). All values obtained from the same simulations as in Fig.~\ref{fig:avgvarloss}.}
	\label{fig:initialtimespace}
\end{figure}

\subsubsection{Comparison with G-pCNN}
A strong inductive bias, e.g. equivariance, to translation or other appropriate symmetries should provide an advantage when learning a mapping equivariant to these same symmetries. Moreover, we expect this advantage to be even more significant in the case of learning with the $\log \text{RN}$ loss. This is because variational rates $\Gamma^{(v)}$ are evaluated multiple times in succession to evaluate the logarithm Radon-Nikodym derivative on a single trajectory, and this is repeated for a whole batch of trajectories.  

Initial training experiments were conducted on the TFIM, comparing the learning of a fully connected network, and the pCNN in 1D, as well as both compared to the group-equivariant CNN in two dimensions. Qualitatively, there is little difference in the performance of the pCNN and the MLP in 1d, whereas the difference becomes more notable in 2d, where the gCNN seems to learn quickest, when comparing networks with comparable numbers of variational parameters. These experiments are limited in two ways. First, due to the stochastic nature of the loss it is hard to tell which architecture is performing better and statistical metrics are needed. Second, because of a suboptimal implementation of the gCNN, runs with many epochs could not have been carried out and the results are thus inconclusive. Further investigation is needed to confirm these qualitative results, and is suggested as future work. 

\section{Importance sampling}
\label{subsec:impsampl}
Ultimately we were unsuccessful in learning the optimal rates $\Gamma^\prime$ in the TFIM model with the $\log {\text{RN}}$ loss. The restrictions that the fixed endpoints and variable length of trajectories put onto the batch construction process are too severe, and the learning algorithm is unable to efficiently learn from the batches constructed in such an artificial way. This section shows that the sampler part of the code was indeed implemented, that sampling can be performed with either the discrete or continuous time formulation of the rates, see Fig~\ref{fig:sampling_tfim1d} left, and that the rates in Fig.~\ref{fig:troubrates} resemble sampling from the passive rates of the TFIM, see Fig.~\ref{fig:sampling_tfim1d} right.

\subsection{Ising model}

\begin{figure}[H]
	\centering
	\subfloat{\includegraphics[width=0.45\linewidth]{Chapter5/Figs/Vector/sampling_example}}
	\quad
	\subfloat{	\includegraphics[width=0.45\linewidth]{Chapter5/Figs/Vector/tfim1d_finite_scaling-justsigz.pdf}}
	\caption[Sampling in the TFIM model]{\textbf{Sampling in the TFIM model.} Convergence of discrete and continuous formulations of the rates towards the expected $\sigma_z$ value, taken as an average of $N=10$ chains (\textbf{left}). In the limited sampling experiments performed,  neither formulation provides a considerable advantage, although the continuous chain is more elegant. Estimation of the modulus per spin magnetisation $M_z=\frac{2}{N}\left|\sum_{i} s_{i}\right|$ using the rates from Fig.~\ref{fig:troubrates}. The analytical solution for $N=\infty$ spins, and a VMC result with NetKet~\cite{netket2019} are shown for comparison. (\textbf{right}).}
	\label{fig:sampling_tfim1d}
\end{figure}

