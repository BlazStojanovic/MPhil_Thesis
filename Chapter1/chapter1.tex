%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}
\label{chapter1}

We are surrounded by intractable problems, and that is even if we consider simplified models of reality. The transverse field Ising model is an example of a simple lattice model that displays very rich physics. Its basic constituents are not particles in continuous space, but spins confined to a grid that only interact with their immediate neighbours and the transverse field. The dynamics of the Ising model can be described by a single vector, but its dimension grows as $2^N$ with the number $N$ of particles, quickly putting us out of our depth. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\linewidth]{Chapter1/ising_passive0}
	\caption[A configuration of the Ising model.]{\textbf{A configuration of the Ising model.}}
	\label{fig:isingpassive0}
\end{figure}
This exponential explosion can be avoided by using stochastic methods, where our goal is to efficiently sample from the probability distribution of spin configurations, which allows us to calculate properties of the system. Needless to say, that for some quantum lattice models lend themselves more naturally to stochastic methods than others. Stoquastic Hamiltonians belong right on the border between statistical and quantum mechanics, and this is precisely where we will operate.

The main objective of this thesis is use concepts from probabilistic machine learning, optimal control, and stochastic processes to devise a method that will be able to learn the Feynman-Kac trajectory distribution in quantum lattice models. Such a method would provide one with the ability to perform optimal importance sampling if a perfect representation of the distribution was found, and a significant reduction in variance for near-optimal representations. 

\section{Thesis Structure}
\label{sec:structure}
The thesis is structured as follows, Chapter~\ref{chapter2} briefly introduces the quantum many-body problem before focusing on numerical solution approaches to it. In addition to standard methods, recent applications of Machine Learning are highlighted. In the first half of Chapter~\ref{chapter3} the focus is turned towards mathematical fundamentals of stochastic processes, which underpin most other parts of the thesis. The rest of the chapter is dedicated to introducing the Feynman-Kac formula and the derivation of the $\log \text{RN}$ loss. The derivation includes the necessary background in optimal control and an analogous derivation in continuous state space, which may be more familiar to some readers. Chapter~\ref{chapter4} describes the methodology and implementation of the method, and Chapter~\ref{chapter5} describes training and sampling experiments conducted. Finally, Chapter~\ref{chapter6} contains concluding remarks.

%********************************** %Chapter 1 Nomenclature **************************************

% Latin expressions and shorthands
\nomenclature[z-0pbc]{p.b.c}{Periodic boundary condition}
\nomenclature[z-0eg]{e.g.}{Exempli gratia ("for the sake of an example")}
\nomenclature[z-0ie]{i.e.}{Id est ("it is")}
\nomenclature[z-0iid]{i.i.d}{Independent and identically distributed}
\nomenclature[z-0st]{s.t.}{Such that}
\nomenclature[z-0wrt]{w.r.t}{With respect to}

% Other symbols
\nomenclature[x-expectation]{$\mathbb{E}$}{Expectation}
\nomenclature[x-variance]{$\operatorname{Var}$}{Variance}
\nomenclature[x-covariance]{$\operatorname{Cov}$}{Covariance}

% Machine Learning Shorthands
\nomenclature[Z-ML]{ML}{Machine Learning}
\nomenclature[Z-DL]{DL}{Deep Learning}
\nomenclature[Z-NN]{NN}{Neural Network}
\nomenclature[Z-DNN]{DNN}{Deep Neural Network}
\nomenclature[Z-CNN1]{CNN}{Convolutional Neural Network}
\nomenclature[Z-CNN]{pCNN}{Periodic Convolutional Neural Network}

% Models Shorthand
\nomenclature[Z-TFIM]{TFIM}{Transverse Field Ising Model}

% Monte Carlo Shorthands
\nomenclature[Z-DFT]{DFT}{Density Functional Theory}
\nomenclature[Z-DMFT]{DMFT}{Dynamical Mean Field Theory}
\nomenclature[Z-DMRG]{DMRG}{Density Matrix Renormalization group}
\nomenclature[Z-MC]{MC}{Monte Carlo}
\nomenclature[Z-QMC]{QMC}{Quantum Monte Carlo}
\nomenclature[Z-VMC]{VMC}{Variational Quantum Monte Carlo}
\nomenclature[Z-DMC]{DMC}{Diffusion Quantum Monte Carlo}
\nomenclature[Z-GFMC]{GFMC}{Green's function Quantum Monte Carlo}

% Other Shorthands
\nomenclature[z-0pdf]{pdf}{Probability density function}
\nomenclature[Z-PDE]{PDE}{Partial Differential Equation}
\nomenclature[Z-QM]{QM}{Quantum Mechanics}

\nomenclature[Z-RN]{RN}{Radon-Nikodym}
\nomenclature[Z-FP]{FP}{Fokker-Planck}
\nomenclature[Z-FK]{FK}{Feynman-Kac}
\nomenclature[Z-SDE]{SDE}{Stochastic Differential Equations}
\nomenclature[Z-0cdf]{cdf}{Cumulative density function}
\nomenclature[Z-0stochprocs]{s.p.}{Stochastic process}
\nomenclature[Z-DTMC]{DTMC}{Discrete time Markov Chain}
\nomenclature[Z-CTMC]{CTMC}{Continuous time Markov Chain}
\nomenclature[Z-SEP]{SEP}{Symmetric Exclusion Process}
\nomenclature[Z-KL]{KL}{Kullback Liebler}
\nomenclature[Z-0mdp]{mdp}{Markov Decision Process}

\nomenclature[x-ratematrix]{$\Gamma$}{Rate matrix}
\nomenclature[x-ratematrix]{$P$}{Transition matrix}
\nomenclature[x-kldiv]{$D_{\mathrm{KL}}$}{Kullback-Liebler divergence}
\nomenclature[x-statespace]{S}{State space of a Markov process}
\nomenclature[x-reals]{$\mathbb{R}$}{The set of real numbers}
\nomenclature[x-normal]{$\mathcal{N}$}{The Gaussian distribution}
\nomenclature[x-measure]{$\mathbb{P}$}{Measure}
\nomenclature[x-filtration]{$\mathbb{F}$}{Filtration}
\nomenclature[x-field]{"$\mathcal{F}$"}{$\sigma$-field (algebra)}
\nomenclature[x-stochprocs]{$\{X_t\}$}{Stochastic process}
\nomenclature[x-randomvariable]{$\{X\}$}{Random variable}
\nomenclature[x-wiener]{$W_t$}{Wiener process, mathematical Brownian motion}

\nomenclature[x-gel]{$\mathfrak{g}$}{Group element}
\nomenclature[x-group]{$\mathfrak{G}$}{Group}
\nomenclature[z-MCMC]{MCMC}{Markov Chain Monte Carlo}
\nomenclature[z-MLP]{MLP}{Multilayer Perceptron}
\nomenclature[z-AD]{AD}{Automatic Differentiation}
\nomenclature[z-gCNN]{G-CNN}{Group Equivariant Convolutional Neural Network}